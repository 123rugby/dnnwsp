{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight sparsity control (Tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is for weight sparsity control in MLP.\n",
    "It is written for Python 3.5/3.6 and Tensorflow 1.1.0.\n",
    "We can easily implement weight sparsity control follwing several steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00. Import "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, call modules which containing Python definitions and statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import scipy.io\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 01. Customization part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "â€» This part would be modified to GUI.  \n",
    "\n",
    "You can change the mode either for classification or autoencoder. => toggle switch  \n",
    "Also, can change weight sparsity control mode in layer wise or node wise. => toggle switch  \n",
    "You may select optimzer algorithm among those five. => five buttons  \n",
    "Default file loaded as dataset is our brain data. but you can load yours later. => file dialog  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "autoencoder or not\n",
    "\"\"\"\n",
    "autoencoder=False\n",
    "\n",
    "\"\"\"\n",
    "Select the weight sparsity control mode\n",
    "'layer' for layer wise sparsity control\n",
    "'node' for node wise sparsity control\n",
    "\"\"\"\n",
    "mode = 'node'\n",
    "\n",
    "\"\"\"\n",
    "Select optimizer\n",
    "'Grad' for GradientDescentOptimizer\n",
    "'Ada' for AdagradOptimizer\n",
    "'Moment' for MomentumOptimizer\n",
    "'Adam' for AdamOptimizer\n",
    "'RMSP' for RMSPropOptimizer\n",
    "\"\"\"\n",
    "optimizer_algorithm='Grad'\n",
    "\n",
    "\"\"\"\n",
    "Load data here\n",
    "\"\"\"\n",
    "dataset = scipy.io.loadmat('/home/hailey/01_study/prni2017_samples/lhrhadvs_sample_data2.mat')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's set the number of nodes the value of first element is for input layer, the last one for output layer,  and the others in the minddle for hidden layers. => input dialog  \n",
    "We can adjust learning parameters total epoch, mini-batch size, when to begin learning rate annealing, decaying rate of learning rate, initial value of learning rate, and minimum value of learning rate. Besides, learning rate of beta of weight sparsity control(for L1 regularization) and L2 parameter(for L2 regularization). =>  input dialog  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Set the number of nodes for input layer, hidden layers, and output layer here\n",
    "\"\"\"\n",
    "nodes=[74484,100,100,100,4]\n",
    "#nodes=[74484,500,74484]\n",
    "\n",
    "\"\"\"\n",
    "Set learning parameters\n",
    "\"\"\"\n",
    "# Set total epoch\n",
    "total_epoch=600\n",
    "# Set mini batch size\n",
    "batch_size=100\n",
    "# Let anealing to begin after 5th epoch\n",
    "beginAnneal=90\n",
    "# anealing decay rate\n",
    "decay_rate=1e-4\n",
    "# Set initial learning rate and minimum                     \n",
    "lr_init = 1e-3    \n",
    "min_lr = 1e-4\n",
    "\n",
    "# Set learning rate of beta for weight sparsity control\n",
    "lr_beta = 0.02\n",
    "# Set L2 parameter for L2 regularization\n",
    "L2_param= 1e-5\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Set maximum beta value of each hidden layer (usually 0.01~0.2) \n",
    "and set target sparsness value (0:dense~1:sparse)\n",
    "\"\"\"\n",
    "max_beta = [0.05, 0.8, 0.8]\n",
    "tg_hsp = [0.7, 0.65, 0.65]\n",
    "\n",
    "#max_beta = [0.02]\n",
    "#tg_hsp = [0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Input data part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data we provides consists of train, test, and validation sets.  \n",
    "This part splits dataset into 3 parts. \n",
    "Input dimension is 74484, and output nodes for classification is 4 (Left hand clenching, right hand clenching, auditory and visual tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the dataset into traning input\n",
    "train_input = dataset['train_x']\n",
    "# Split the dataset into test input\n",
    "test_input = dataset['test_x']\n",
    "\n",
    "\n",
    "# Split the dataset into traning output \n",
    "train_output = np.zeros((np.shape(dataset['train_y'])[0],np.max(dataset['train_y'])+1))\n",
    "# trainsform classes into One-hot\n",
    "for i in np.arange(np.shape(dataset['train_y'])[0]):\n",
    "    train_output[i][dataset['train_y'][i][0]]=1 \n",
    "dataset['train_y']\n",
    "\n",
    "# Split the dataset into test output\n",
    "test_output = np.zeros((np.shape(dataset['test_y'])[0],np.max(dataset['test_y'])+1))\n",
    "# trainsform classes into One-hot\n",
    "for i in np.arange(np.shape(dataset['test_y'])[0]):\n",
    "    test_output[i][dataset['test_y'][i][0]]=1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. Structure part "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part will build our MLP model by concatenating layers we've made before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# We need 'node_index' for split placeholder (hidden_nodes=[100, 100, 100] -> nodes_index=[0, 100, 200, 300])\n",
    "nodes_index= [int(np.sum(nodes[1:i+1])) for i in np.arange(np.shape(nodes)[0]-1)]\n",
    "\n",
    "# Make placeholders to make our model in advance, then fill the values later when training or testing\n",
    "X=tf.placeholder(tf.float32,[None,nodes[0]])\n",
    "Y=tf.placeholder(tf.float32,[None,nodes[-1]])\n",
    "\n",
    "# Make weight variables which are randomly initialized\n",
    "w_init=[tf.div(tf.random_normal([nodes[i],nodes[i+1]]), tf.sqrt(float(nodes[i])/2)) for i in np.arange(np.shape(nodes)[0]-1)]\n",
    "w=[tf.Variable(w_init[i], dtype=tf.float32) for i in np.arange(np.shape(nodes)[0]-1)]\n",
    "# Make bias variables which are randomly initialized\n",
    "b=[tf.Variable(tf.random_normal([nodes[i+1]])) for i in np.arange(np.shape(nodes)[0]-1)]\n",
    "\n",
    "# Finally build our DNN model \n",
    "hidden_layers=[0.0]*(np.shape(nodes)[0]-2)\n",
    "for i in np.arange(np.shape(nodes)[0]-2):\n",
    "    \n",
    "    # Input layer\n",
    "    if i==0:\n",
    "        hidden_layers[i]=tf.add(tf.matmul(X,w[i]),b[i])\n",
    "        hidden_layers[i]=tf.nn.tanh(hidden_layers[i])\n",
    "        \n",
    "    # The other layers    \n",
    "    else:     \n",
    "        hidden_layers[i]=tf.add(tf.matmul(hidden_layers[i-1],w[i]),b[i])\n",
    "        hidden_layers[i]=tf.nn.tanh(hidden_layers[i])\n",
    "\n",
    "output_layer=tf.add(tf.matmul(hidden_layers[-1],w[-1]),b[-1])\n",
    "logRegression_layer=tf.nn.tanh(output_layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. Learning part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of functions to create\n",
    "- beta_vec\n",
    "- L1 loss\n",
    "- cost\n",
    "- optimzer\n",
    "- sparsity control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Make placeholders for total beta vectors (make a long one to concatenate every beta vector) \n",
    "def build_betavec():\n",
    "    if mode=='layer':\n",
    "        Beta_vec=tf.placeholder(tf.float32,[np.shape(nodes)[0]-2])\n",
    "    elif mode=='node':\n",
    "        Beta_vec=tf.placeholder(tf.float32,[np.sum(nodes[1:-1])])\n",
    "\n",
    "    return Beta_vec\n",
    "\n",
    "\n",
    "# Make L1 loss term and L2 loss term for regularisation\n",
    "def build_L1loss():\n",
    "    if mode=='layer':\n",
    "        L1_loss=[Beta_vec[i]*tf.reduce_sum(abs(w[i])) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "    elif mode=='node':\n",
    "        L1_loss=[tf.reduce_sum(tf.matmul(abs(w[i]),tf.cast(tf.diag(Beta_vec[nodes_index[i]:nodes_index[i+1]]),tf.float32))) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "\n",
    "    return L1_loss\n",
    "\n",
    "       \n",
    "\n",
    "# Define cost term with cross entropy and L1 and L2 tetm     \n",
    "def build_cost():\n",
    "    if autoencoder:\n",
    "        cost=tf.reduce_mean(tf.pow(X - output_layer, 2)) + tf.reduce_sum(L1_loss) + L2_param*tf.reduce_sum(L2_loss)\n",
    "    else:\n",
    "        cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logRegression_layer, labels=Y)) \\\n",
    "                                         + tf.reduce_sum(L1_loss) + L2_param*tf.reduce_sum(L2_loss)                                         \n",
    "    return cost\n",
    "\n",
    "\n",
    "# Define optimizer\n",
    "def build_optimizer(Lr):\n",
    "    if optimizer_algorithm=='Grad':\n",
    "        optimizer=tf.train.GradientDescentOptimizer(Lr).minimize(cost) \n",
    "    elif optimizer_algorithm=='Ada':\n",
    "        optimizer=tf.train.AdagradOptimizer(Lr).minimize(cost) \n",
    "    elif optimizer_algorithm=='Adam':\n",
    "        optimizer=tf.train.AdamOptimizer(Lr).minimize(cost) \n",
    "    elif optimizer_algorithm=='Moment':\n",
    "        optimizer=tf.train.MomentumOptimizer(Lr).minimize(cost) \n",
    "    elif optimizer_algorithm=='RMSP':\n",
    "        optimizer=tf.train.RMSPropOptimizer(Lr).minimize(cost) \n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "\n",
    "if mode=='layer':\n",
    "    # Weight sparsity control with Hoyer's sparsness (Layer wise)  \n",
    "    def Hoyers_sparsity_control(w_,b,max_b,tg):\n",
    "        \n",
    "        # Get value of weight\n",
    "        W=sess.run(w_)\n",
    "        [nodes,dim]=W.shape  \n",
    "        num_elements=nodes*dim\n",
    " \n",
    "        Wvec=W.flatten()\n",
    "        \n",
    "        # Calculate L1 and L2 norm     \n",
    "        L1=LA.norm(Wvec,1)\n",
    "        L2=LA.norm(Wvec,2)\n",
    "        \n",
    "        # Calculate hoyer's sparsness\n",
    "        h=(np.sqrt(num_elements)-(L1/L2))/(np.sqrt(num_elements)-1)\n",
    "        \n",
    "        # Update beta\n",
    "        b-=lr_beta*np.sign(h-tg)\n",
    "        \n",
    "        # Trim value\n",
    "        b=0.0 if b<0.0 else b\n",
    "        b=max_b if b>max_b else b\n",
    "                         \n",
    "        return [h,b]\n",
    "    \n",
    "    \n",
    "elif mode=='node':   \n",
    "    # Weight sparsity control with Hoyer's sparsness (Node wise)\n",
    "    def Hoyers_sparsity_control(w_,b_vec,max_b,tg):\n",
    "    \n",
    "        # Get value of weight\n",
    "        W=sess.run(w_)\n",
    "        [nodes,dim]=W.shape\n",
    "        \n",
    "        # Calculate L1 and L2 norm \n",
    "        L1=LA.norm(W,1,axis=0)\n",
    "        L2=LA.norm(W,2,axis=0)\n",
    "        \n",
    "        h_vec = np.zeros((1,dim))\n",
    "        tg_vec = np.ones(dim)*tg\n",
    "        \n",
    "        # Calculate hoyer's sparsness\n",
    "        h_vec=(np.sqrt(nodes)-(L1/L2))/(np.sqrt(nodes)-1)\n",
    "        \n",
    "        # Update beta\n",
    "        b_vec-=lr_beta*np.sign(h_vec-tg_vec)\n",
    "        \n",
    "        # Trim value\n",
    "        b_vec[b_vec<0.0]=0.0\n",
    "        b_vec[b_vec>max_b]=max_b\n",
    "        \n",
    "               \n",
    "        return [h_vec,b_vec]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create them by using those functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "lr = lr_init\n",
    "\n",
    "Beta_vec = build_betavec()\n",
    "\n",
    "L1_loss = build_L1loss()\n",
    "L2_loss = [tf.reduce_sum(tf.square(w[i])) for i in np.arange(np.shape(nodes)[0]-1)] \n",
    "\n",
    "cost = build_cost()\n",
    "\n",
    "\n",
    "# Make learning rate as placeholder to update learning rate every iterarion \n",
    "Lr=tf.placeholder(tf.float32)\n",
    "optimizer=build_optimizer(Lr)\n",
    "  \n",
    "\n",
    "if not autoencoder:\n",
    "    correct_prediction=tf.equal(tf.argmax(output_layer,1),tf.argmax(Y,1))  \n",
    "        \n",
    "    # calculate mean error(accuracy) depending on the frequency it predicts correctly   \n",
    "    error=1-tf.reduce_mean(tf.cast(correct_prediction,tf.float32))      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05. Condition check part "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check conditions to catch some possible errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "condition=False\n",
    "\n",
    "print()\n",
    "\n",
    "if np.shape(nodes)[0] <3:\n",
    "    print(\"Error : Not enough hidden layer number.\")\n",
    "elif (autoencoder==False) & (np.shape(train_input)[0] != np.shape(train_output)[0]):\n",
    "    print(\"Error : The sizes of input train dataset and output train dataset don't match. \")  \n",
    "elif (autoencoder==False) & (np.shape(test_input)[0] != np.shape(test_output)[0]):\n",
    "    print(\"Error : The sizes of input test dataset and output test dataset don't match. \")     \n",
    "elif not ((mode=='layer') | (mode=='node')):\n",
    "    print(\"Error : Select a valid mode. \") \n",
    "elif (np.any(np.array(tg_hsp)<0)) | (np.any(np.array(tg_hsp)>1)):  \n",
    "    print(\"Error : Please set the target sparsities appropriately.\")\n",
    "elif autoencoder!=True & autoencoder!=False:\n",
    "    print(\"Error : Please set the autoencoder mode appropriately.\")\n",
    "else:\n",
    "    condition=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06. Training & test part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If condition is met, start session where training and tesing would be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "if condition==True:\n",
    "    \n",
    "    # make initializer        \n",
    "    init = tf.global_variables_initializer()              \n",
    "\n",
    "    \n",
    "    \n",
    "    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "        \n",
    "        # run tensorflow variable initialization\n",
    "        sess.run(init)\n",
    "    \n",
    "        # initialization    \n",
    "        def initialization():           \n",
    "            if mode=='layer': \n",
    "                beta=np.zeros(np.shape(nodes)[0]-2)\n",
    "                beta_vec = np.zeros(np.shape(nodes)[0]-2)\n",
    "                hsp = np.zeros(np.shape(nodes)[0]-2)            \n",
    "                plot_beta = np.zeros(np.shape(nodes)[0]-2)\n",
    "                plot_hsp = np.zeros(np.shape(nodes)[0]-2)\n",
    "                           \n",
    "            elif mode=='node':                       \n",
    "                beta = [np.zeros(nodes[i+1]) for i in np.arange(np.shape(nodes)[0]-2)]  \n",
    "                beta_vec=np.zeros(np.sum(nodes[1:-1]))\n",
    "                hsp = [np.zeros(nodes[i+1]) for i in np.arange(np.shape(nodes)[0]-2)]            \n",
    "                plot_beta = [np.zeros(nodes[i+1]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "                plot_hsp = [np.zeros(nodes[i+1]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "                \n",
    "            # make arrays to plot results\n",
    "            plot_lr=np.zeros(1)\n",
    "            plot_cost=np.zeros(1)\n",
    "            plot_train_err=np.zeros(1)\n",
    "            plot_test_err=np.zeros(1)\n",
    "            \n",
    "            return beta, beta_vec, hsp, plot_beta, plot_hsp, plot_lr, plot_cost, plot_train_err, plot_test_err\n",
    "                \n",
    "        beta, beta_vec, hsp, plot_beta, plot_hsp, plot_lr, plot_cost, plot_train_err, plot_test_err = initialization()\n",
    "        \n",
    "               \n",
    "\n",
    "        \n",
    "           \n",
    "        # Calculate how many mini-batch iterations\n",
    "        total_batch=int(np.shape(train_input)[0]/batch_size) \n",
    "               \n",
    "        # train and get cost\n",
    "        cost_avg=0.0\n",
    "        for epoch in np.arange(total_epoch):            \n",
    "            cost_epoch=0.0\n",
    "            \n",
    "            # Begin Annealing\n",
    "            if beginAnneal == 0:\n",
    "                lr = lr * 1.0\n",
    "            elif epoch+1 > beginAnneal:\n",
    "                lr = max( min_lr, (-decay_rate*(epoch+1) + (1+decay_rate*beginAnneal)) * lr )  \n",
    "            \n",
    "            \n",
    "            # Train at each mini batch    \n",
    "            for batch in np.arange(total_batch):\n",
    "                batch_x = train_input[batch*batch_size:(batch+1)*batch_size]\n",
    "                batch_y = train_output[batch*batch_size:(batch+1)*batch_size]\n",
    "                \n",
    "                # Get cost and optimize the model\n",
    "                if autoencoder:\n",
    "                    cost_batch,_=sess.run([cost,optimizer],feed_dict={Lr:lr, X:batch_x, Beta_vec:beta_vec })\n",
    "                else:                   \n",
    "                    cost_batch,_=sess.run([cost,optimizer],feed_dict={Lr:lr, X:batch_x, Y:batch_y, Beta_vec:beta_vec })\n",
    "                    \n",
    "                cost_epoch+=cost_batch/total_batch      \n",
    "        \n",
    "                \n",
    "                \n",
    "        \n",
    "                # run weight sparsity control function\n",
    "                for i in np.arange(np.shape(nodes)[0]-2):\n",
    "                    [hsp[i],beta[i]]=Hoyers_sparsity_control(w[i], beta[i], max_beta[i], tg_hsp[i])   \n",
    "                    \n",
    "                if mode=='layer':               \n",
    "                    beta_vec=beta                      \n",
    "                elif mode=='node':                              \n",
    "                    beta_vec=[item for sublist in beta for item in sublist]\n",
    "                    \n",
    "            train_err_epoch=sess.run(error,feed_dict={X:train_input, Y:train_output})\n",
    "            plot_train_err=np.hstack([plot_train_err,[train_err_epoch]])\n",
    "            \n",
    "            test_err_epoch=sess.run(error,feed_dict={X:test_input, Y:test_output})\n",
    "            plot_test_err=np.hstack([plot_test_err,[test_err_epoch]])\n",
    "            \n",
    "            \n",
    "            # make space to plot beta, sparsity level\n",
    "            plot_lr=np.hstack([plot_lr,[lr]])\n",
    "            plot_cost=np.hstack([plot_cost,[cost_epoch]])\n",
    "\n",
    "            \n",
    "            # save footprint for plot\n",
    "            if mode=='layer':\n",
    "                plot_hsp=[np.vstack([plot_hsp[i],[hsp[i]]]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "                plot_beta=[np.vstack([plot_beta[i],[beta_vec[i]]]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "                \n",
    "            elif mode=='node':\n",
    "                plot_hsp=[np.vstack([plot_hsp[i],[np.transpose(hsp[i])]]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "                plot_beta=[np.vstack([plot_beta[i],[np.transpose(beta[i])]]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "\n",
    "            \n",
    "                    \n",
    "            # Print cost at each epoch        \n",
    "            print(\"< Epoch\", \"{:02d}\".format(epoch+1),\"> Cost : \", \"{:.4f}\".format(cost_epoch))\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        # Print final accuracy of test set\n",
    "        if not autoencoder:\n",
    "            print(\"Accuracy :\",1-test_err_epoch)\n",
    "            \n",
    "else:\n",
    "    # Don't run the sesstion but print 'failed' if any condition is unmet\n",
    "    print(\"Failed!\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07. Plot & save results part "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, show the the results and save them as .mat file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "if condition==True:\n",
    "       \n",
    "    # Plot the change of learning rate\n",
    "    plt.title(\"Learning rate plot\",fontsize=16)\n",
    "    plot_lr=plot_lr[1:]\n",
    "    plt.ylim(0.0, lr_init*1.2)\n",
    "    plt.plot(plot_lr)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the change of cost\n",
    "    plt.title(\"Cost plot\",fontsize=16)\n",
    "    plot_cost=plot_cost[1:]\n",
    "    plt.plot(plot_cost)\n",
    "    plt.yscale('log')\n",
    "    plt.show()     \n",
    "    \n",
    "    \n",
    "\n",
    "    # Plot test error\n",
    "    plt.title(\"Training & Test error\",fontsize=16)\n",
    "    plot_test_err=plot_test_err[1:]\n",
    "    plt.plot(plot_train_err)\n",
    "    plt.hold\n",
    "    plt.plot(plot_test_err)\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    plt.legend(['Training error', 'Test error'],loc='upper right')\n",
    "#    plt.yscale('log')\n",
    "    plt.show() \n",
    "\n",
    " \n",
    "    \n",
    "    \n",
    "    # Plot the change of beta value\n",
    "    print(\"\")       \n",
    "    for i in np.arange(np.shape(nodes)[0]-2):\n",
    "        print(\"\")\n",
    "        print(\"                  < Hidden layer\",i+1,\">\")\n",
    "        plt.title(\"Beta plot\",fontsize=16)\n",
    "        plot_beta[i]=plot_beta[i][1:]\n",
    "        plt.plot(plot_beta[i])\n",
    "        plt.ylim(0.0, np.max(max_beta)*1.2)\n",
    "        plt.show()\n",
    "    \n",
    "    # Plot the change of Hoyer's sparsness\n",
    "    print(\"\")            \n",
    "    for i in np.arange(np.shape(nodes)[0]-2):\n",
    "        print(\"\")\n",
    "        print(\"                  < Hidden layer\",i+1,\">\")\n",
    "        plt.title(\"Hoyer's sparsness plot\",fontsize=16)\n",
    "        plot_hsp[i]=plot_hsp[i][1:]\n",
    "        plt.plot(plot_hsp[i])\n",
    "        plt.ylim(0.0, 1.0)\n",
    "        plt.show()\n",
    "    \n",
    "    # make a new 'results' directory in the current directory\n",
    "    current_directory = os.getcwd()\n",
    "    final_directory = os.path.join(current_directory, r'results')\n",
    "    if not os.path.exists(final_directory):\n",
    "        os.makedirs(final_directory) \n",
    "        \n",
    "    # save results as .mat file\n",
    "    scipy.io.savemat(\"results/result_learningrate.mat\", mdict={'lr': plot_lr})\n",
    "    scipy.io.savemat(\"results/result_cost.mat\", mdict={'cost': plot_cost})\n",
    "    scipy.io.savemat(\"results/result_beta.mat\", mdict={'beta': plot_beta})\n",
    "    scipy.io.savemat(\"results/result_hsp.mat\", mdict={'hsp': plot_hsp})\n",
    "\n",
    "else:\n",
    "    None "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
