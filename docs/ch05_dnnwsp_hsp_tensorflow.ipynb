{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight sparsity control (Tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is written for Python 3.5/3.6 and Tensorflow 1.1.0.  \n",
    "This code is for weight sparsity control in MLP. You may easily implement weight sparsity control follwing several steps described below.  \n",
    "\n",
    "00. Import  \n",
    "    - Importing modules \n",
    "01. Parameters\n",
    "    - 'mode, optimizer_algorithm, nodes, total_epoch, batch_size, beginAnneal, decay_rate, lr_init, min_lr,lr_beta, L2_param, max_beta, tg_hsp' on GUI \n",
    "02. Input data\n",
    "    - Train and test data set from sample data set\n",
    "03. Build Model\n",
    "    - Building model with L1 and L2 regulariation term using trainig data and test model using test data\n",
    "04. Variables declaration\n",
    "    - beta(L1 reg parameter), L1 & L2 regularzation, cost, optimizer, other variables, and Hoyer's sparsity control function.\n",
    "05. Condition check\n",
    "    - Check the conditions to catch some possible errors.\n",
    "06. Learning\n",
    "    - Train the model\n",
    "07. Plot & save results\n",
    "    - plot the results and save them as .mat file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00. Import "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, call modules which containing Python definitions and statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This import statement gives Python access to all of TensorFlow's classes, methods, and symbols. \n",
    "import tensorflow as tf\n",
    "# NumPy is the fundamental package for scientific computing with Python.\n",
    "import numpy as np\n",
    "# Linear algebra module for calculating L1 and L2 norm  \n",
    "from numpy import linalg as LA\n",
    "# To plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "# To check the directory when saving the results\n",
    "import os.path\n",
    "# The module for file input and output\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 01. Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "When we run this part, GUI would start where we can input some paramteres.\n",
    "We can specify either using weight sparsity control mode in layer wise or node wise, and also may select optimzer algorithm among five selections.  \n",
    "Then set the number of nodes the value of first element is for input layer, the last one for output layer,  and the others in the middle for hidden layers.  \n",
    "We can adjust learning parameters total epoch, mini-batch size, when to begin learning rate annealing, decaying rate of learning rate, initial value of learning rate, and minimum value of learning rate. Besides, learning rate of beta of weight sparsity control(for L1 regularization) and L2 regularization parameter(for L2 regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from customizationGUI \\\n",
    "        import mode, optimizer_algorithm, nodes, n_epochs, batch_size,\\\n",
    "        beginAnneal, decay_rate, lr_init, lr_min, beta_lrates, L2_reg, max_beta, tg_hspset\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset we provides consists of train, test, and validation parts. We will use train and test parts. Then here we split dataset into train_x, train_y, test_x and test_y.  \n",
    "Input dimension is 74484, and the number of output nodes for classification is four (Left-hand clecnhing (LH), right-hand clecnhing, auditory attention (AD), and visual stimulus (VS) tasks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "datasets = sio.loadmat('lhrhadvs_sample_data.mat')\n",
    "\n",
    "################ lhrhadvs_sample_data.mat ##################\n",
    "# train_x  = 240 volumes x 74484 voxels  \n",
    "# train_y  = 240 volumes x 1 [0:LH, 1:RH, 2:AD, 3:VS]\n",
    "# test_x  = 120 volumes x 74484 voxels\n",
    "# test_y  = 120 volumes x 1 [0:LH, 1:RH, 2:AD, 3:VS]\n",
    "############################################################\n",
    "\n",
    "\n",
    "train_x = datasets['train_x']\n",
    "train_y = np.zeros((np.shape(datasets['train_y'])[0],np.max(datasets['train_y'])+1))\n",
    "# transform into One-hot\n",
    "for i in np.arange(np.shape(datasets['train_y'])[0]):\n",
    "    train_y[i][datasets['train_y'][i][0]]=1 \n",
    "\n",
    "\n",
    "test_x = datasets['test_x']\n",
    "test_y = np.zeros((np.shape(datasets['test_y'])[0],np.max(datasets['test_y'])+1))\n",
    "# transform into One-hot\n",
    "for i in np.arange(np.shape(datasets['test_y'])[0]):\n",
    "    test_y[i][datasets['test_y'][i][0]]=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part builds the MLP model by concatenating all layers based on the information we provided earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 'node_index' to split placeholder, for an example, given hidden_nodes=[100, 100, 100], nodes_index=[0, 100, 200, 300]\n",
    "nodes_index= [int(np.sum(nodes[1:i+1])) for i in np.arange(np.shape(nodes)[0]-1)]\n",
    "\n",
    "# Make two placeholders to fill the values later when training or testing\n",
    "X=tf.placeholder(tf.float32,[None,nodes[0]])\n",
    "Y=tf.placeholder(tf.float32,[None,nodes[-1]])\n",
    "\n",
    "# Create randomly initialized weight variables \n",
    "w_init=[tf.div(tf.random_normal([nodes[i],nodes[i+1]]), tf.sqrt(float(nodes[i])/2)) for i in np.arange(np.shape(nodes)[0]-1)]\n",
    "w=[tf.Variable(w_init[i], dtype=tf.float32) for i in np.arange(np.shape(nodes)[0]-1)]\n",
    "# Create randomly initialized bias variables \n",
    "b=[tf.Variable(tf.random_normal([nodes[i+1]]), dtype=tf.float32) for i in np.arange(np.shape(nodes)[0]-1)]\n",
    "\n",
    "# Build MLP model \n",
    "hidden_layers=[0.0]*(np.shape(nodes)[0]-2)\n",
    "for i in np.arange(np.shape(nodes)[0]-2):\n",
    "    # Input layer\n",
    "    if i==0:\n",
    "        hidden_layers[i]=tf.add(tf.matmul(X,w[i]),b[i])\n",
    "        hidden_layers[i]=tf.nn.tanh(hidden_layers[i])\n",
    "    # The other layers    \n",
    "    else:     \n",
    "        hidden_layers[i]=tf.add(tf.matmul(hidden_layers[i-1],w[i]),b[i])\n",
    "        hidden_layers[i]=tf.nn.tanh(hidden_layers[i])\n",
    "# Output layer\n",
    "output_layer=tf.add(tf.matmul(hidden_layers[-1],w[-1]),b[-1])\n",
    "\n",
    "# Logistic regression layer\n",
    "logRegression_layer=tf.nn.tanh(output_layer)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. Variables declaration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitions of functions to create and initialize\n",
    "- beta (L1 reg parameter)\n",
    "- L1 regularzation\n",
    "- L2 regularzation\n",
    "- cost\n",
    "- optimizer : GradientDescent, Adagrad, Adam... \n",
    "- other variables : lr, beta_val, beta, hsp_val, plot_beta...\n",
    "- sparsity control function : layer wise of node wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Make placeholders for total beta array (make a long one to concatenate every beta vector) \n",
    "def init_beta():\n",
    "    if mode=='layer':\n",
    "        # The size is same with the number of layers\n",
    "        Beta=tf.placeholder(tf.float32,[np.shape(nodes)[0]-2])\n",
    "    elif mode=='node':\n",
    "        # The size is same with the number of nodes\n",
    "        Beta=tf.placeholder(tf.float32,[np.sum(nodes[1:-1])])\n",
    "\n",
    "    return Beta\n",
    "\n",
    "\n",
    "# Make L1 loss term for regularization\n",
    "def init_L1loss():\n",
    "    if mode=='layer':\n",
    "        # Get L1 loss term by simply multiplying beta(scalar value) and L1 norm of weight for each layer\n",
    "        L1_loss=[Beta[i]*tf.reduce_sum(abs(w[i])) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "    elif mode=='node':\n",
    "        # Get L1 loss term by multiplying beta(vector values as many as nodes) and L1 norm of weight for each layer\n",
    "        L1_loss=[tf.reduce_mean(tf.matmul(abs(w[i]),tf.cast(tf.diag(Beta[nodes_index[i]:nodes_index[i+1]]),tf.float32))) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "        \n",
    "    L1_loss_total=tf.reduce_sum(L1_loss)\n",
    "\n",
    "    return L1_loss_total\n",
    "\n",
    "\n",
    "# Make L2 loss term for regularization\n",
    "def init_L2loss():\n",
    "    L2_loss=[tf.reduce_sum(tf.square(w[i])) for i in np.arange(np.shape(nodes)[0]-1)] \n",
    "    \n",
    "    L2_loss_total=L2_reg*tf.reduce_sum(L2_loss) \n",
    "    \n",
    "    return L2_loss_total\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "# Define cost term (Cost = cross entropy + L1 term + L2 term )    \n",
    "def init_cost():\n",
    "\n",
    "    # A softmax regression : it adds up the evidence of our input being in certain classes, and converts that evidence into probabilities.\n",
    "    cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logRegression_layer, labels=Y)) \\\n",
    "                                     + L1_loss_total + L2_loss_total \n",
    "    \n",
    "    return cost\n",
    "\n",
    "\n",
    "# TensorFlow provides optimizers that slowly change each variable in order to minimize the loss function.\n",
    "def init_optimizer(Lr):\n",
    "    if optimizer_algorithm=='GradientDescent':\n",
    "        optimizer=tf.train.GradientDescentOptimizer(Lr).minimize(cost) \n",
    "    elif optimizer_algorithm=='Adagrad':\n",
    "        optimizer=tf.train.AdagradOptimizer(Lr).minimize(cost) \n",
    "    elif optimizer_algorithm=='Adam':\n",
    "        optimizer=tf.train.AdamOptimizer(Lr).minimize(cost) \n",
    "    elif optimizer_algorithm=='Momentum':\n",
    "        optimizer=tf.train.MomentumOptimizer(Lr).minimize(cost) \n",
    "    elif optimizer_algorithm=='RMSProp':\n",
    "        optimizer=tf.train.RMSPropOptimizer(Lr).minimize(cost) \n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "\n",
    "# initialization   \n",
    "def init_otherVariables():           \n",
    "    if mode=='layer': \n",
    "        beta_val = np.zeros(np.shape(nodes)[0]-2)\n",
    "        beta = np.zeros(np.shape(nodes)[0]-2)\n",
    "        hsp_val = np.zeros(np.shape(nodes)[0]-2)            \n",
    "        plot_beta = np.zeros(np.shape(nodes)[0]-2)\n",
    "        plot_hsp = np.zeros(np.shape(nodes)[0]-2)\n",
    "                   \n",
    "    elif mode=='node':                       \n",
    "        beta_val = [np.zeros(nodes[i+1]) for i in np.arange(np.shape(nodes)[0]-2)]  \n",
    "        beta = np.zeros(np.sum(nodes[1:-1]))\n",
    "        hsp_val = [np.zeros(nodes[i+1]) for i in np.arange(np.shape(nodes)[0]-2)]            \n",
    "        plot_beta = [np.zeros(nodes[i+1]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "        plot_hsp = [np.zeros(nodes[i+1]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "    \n",
    "    # make arrays to store and plot results\n",
    "    plot_lr=np.zeros(1)\n",
    "    plot_cost=np.zeros(1)\n",
    "    plot_train_err=np.zeros(1)\n",
    "    plot_test_err=np.zeros(1)\n",
    "    \n",
    "    # initialize learning rate\n",
    "    lr = lr_init \n",
    "    \n",
    "    \n",
    "    return lr, beta_val, beta, hsp_val, plot_beta, plot_hsp, plot_lr, plot_cost, plot_train_err, plot_test_err\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create them by using those functions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Make a placeholder to be able to update learning rate (Learning rate decaying) \n",
    "Lr=tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "Beta = init_beta()\n",
    "L1_loss_total = init_L1loss()\n",
    "L2_loss_total = init_L2loss()\n",
    "cost = init_cost()\n",
    "\n",
    "optimizer=init_optimizer(Lr)\n",
    "\n",
    " \n",
    "correct_prediction=tf.equal(tf.argmax(output_layer,1),tf.argmax(Y,1))  \n",
    "# calculate an average error depending on how frequent it classified correctly   \n",
    "error=1-tf.reduce_mean(tf.cast(correct_prediction,tf.float32))      \n",
    "\n",
    "\n",
    "lr, beta_val, beta, hsp_val, plot_beta, plot_hsp, plot_lr, plot_cost, plot_train_err, plot_test_err = init_otherVariables()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Beta update (Layer-wise control of weight sparsity via Hoyer sparseness)\n",
    "<img style=\"float: left;\" src=\"https://mail.google.com/mail/u/0/?ui=2&ik=c50bcb3be9&view=fimg&th=15caffbdd1b6872b&attid=0.1&disp=emb&realattid=ii_15caffbd16a91e68&attbid=ANGjdJ_WKYURGF8ttBGH6DX2X-77Z_rVR5m0liygsMxaWNQdH6fuQbkr0wKwClk_E8BNwIohKpzNNsM-y3hDueTHQ84QvMg7E2p6HtPaaBmzmbcy1OdOoeV_RHW7iDY&sz=w722-h136&ats=1497601139858&rm=15caffbdd1b6872b&zw&atsh=1\"> <br clear=\"all\" />  <br clear=\"all\" /> \n",
    "\n",
    "- Beta update (Node-wise control of weight sparsity via Hoyer sparseness)   \n",
    "<img style=\"float: left;\" src=\"https://mail.google.com/mail/u/0/?ui=2&ik=c50bcb3be9&view=fimg&th=15caffbdd1b6872b&attid=0.2&disp=emb&realattid=ii_15caffbd33376b1e&attbid=ANGjdJ8xQ8zAbR80xHjdzK8IATgLIBUiDd1s3v_5ZNTQw3-gdgBMPy48QE3o65GuQmfpGdmLwD-ZSiy5LLi3VuoWWMcrfGALEdBBWPgSDbehADqmCH2GJPcsm5ksO6I&sz=w788-h120&ats=1497601139858&rm=15caffbdd1b6872b&zw&atsh=1\"> <br clear=\"all\" />  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if mode=='layer':\n",
    "    # Weight sparsity control with Hoyer's sparsness (Layer wise)  \n",
    "    def Hoyers_sparsity_control(w_,b,max_b,tg):\n",
    "        \n",
    "        # Get value of weight\n",
    "        W=sess.run(w_)\n",
    "        [nodes,dim]=W.shape  \n",
    "        \n",
    "        # vectorize weight matrix \n",
    "        Wvec=W.flatten()     \n",
    "        sqrt_nsamps=np.sqrt(Wvec.shape[0])\n",
    "        \n",
    "        # Calculate L1 and L2 norm     \n",
    "        L1=LA.norm(Wvec,1)\n",
    "        L2=LA.norm(Wvec,2)\n",
    "        \n",
    "        # Calculate hoyer's sparsness\n",
    "        h=(sqrt_nsamps-(L1/L2))/(sqrt_nsamps-1)\n",
    "        \n",
    "        # Update beta\n",
    "        b-=beta_lrates*np.sign(h-tg)\n",
    "        \n",
    "        # Trim value\n",
    "        b=0.0 if b<0.0 else b\n",
    "        b=max_b if b>max_b else b\n",
    "                         \n",
    "        return [h,b]\n",
    "    \n",
    "    \n",
    "elif mode=='node':   \n",
    "    # Weight sparsity control with Hoyer's sparsness (Node wise)\n",
    "    def Hoyers_sparsity_control(w_,b_vec,max_b,tg):\n",
    "    \n",
    "        # Get value of weight\n",
    "        W=sess.run(w_)\n",
    "        [nodes,dim]=W.shape\n",
    "        sqrt_nsamps=np.sqrt(nodes)\n",
    "        \n",
    "        # Calculate L1 and L2 norm \n",
    "        L1=LA.norm(W,1,axis=0)\n",
    "        L2=LA.norm(W,2,axis=0)\n",
    "     \n",
    "        # Calculate hoyer's sparsness\n",
    "        h_vec = np.zeros((1,dim))\n",
    "        h_vec=(sqrt_nsamps-(L1/L2))/(sqrt_nsamps-1)\n",
    "        \n",
    "        tg_vec = np.ones(dim)*tg\n",
    "        # Update beta       \n",
    "        b_vec-=beta_lrates*np.sign(h_vec-tg_vec)\n",
    "        \n",
    "        # Trim value\n",
    "        b_vec[b_vec<0.0]=0.0\n",
    "        b_vec[b_vec>max_b]=max_b\n",
    "        \n",
    "               \n",
    "        return [h_vec,b_vec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05. Condition check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the conditions to catch some possible errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "condition=False\n",
    "\n",
    "print()\n",
    "\n",
    "if np.size(nodes) <3:\n",
    "    print(\"Error : The number of total layers is not enough.\")\n",
    "elif (np.size(nodes)-2) != np.size(max_beta):\n",
    "    print(\"Error : The number of hidden layers and max beta values don't match. \")\n",
    "elif (np.size(nodes)-2) != np.size(tg_hspset):\n",
    "    print(\"Error : The number of hidden layers and target sparsity values don't match.\")\n",
    "elif np.size(train_x,axis=0) != np.size(train_y,axis=0):\n",
    "    print(\"Error : The sizes of input train datasets and output train datasets don't match. \")  \n",
    "elif np.size(test_x,axis=0) != np.size(test_y,axis=0):\n",
    "    print(\"Error : The sizes of input test datasets and output test datasets don't match. \")     \n",
    "elif (np.any(np.array(tg_hspset)<0)) | (np.any(np.array(tg_hspset)>1)):  \n",
    "    print(\"Error : The values of target sparsities are inappropriate.\")\n",
    "else:\n",
    "    condition=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06. Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If condition is satisfied, may start session where training and tesing proceed.\n",
    "Firstly, initialize variables with initialization function.\n",
    "Then start learning , getting cost and optimizing, for all epochs. In every epoch, training data is split into mini batches so that every learning iteration is mini batch learning.\n",
    "At the end of every epoch, get training error and test error. Also, save cost, learning rate, beta, hsp and so on in order to plot them later. \n",
    "\n",
    "※ No need for 'feed_dict=' on Tensorflow version 1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "if condition==True:\n",
    "    \n",
    "\n",
    "    # variables are not initialized when you call tf.Variable\n",
    "    # To initialize all the variables in a TensorFlow program, you must explicitly call a special operation         \n",
    "    init = tf.global_variables_initializer()              \n",
    "     \n",
    "\n",
    "    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:           \n",
    "    \n",
    "        # run tensorflow variable initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "\n",
    "        # Start training \n",
    "        for epoch in np.arange(n_epochs):            \n",
    "                   \n",
    "            # Shuffle training data at the begining of each epoch           \n",
    "            total_sample = np.size(train_x, axis=0)\n",
    "            sample_ids = np.arange(total_sample)\n",
    "            np.random.shuffle(sample_ids) \n",
    "            \n",
    "            train_x_shuff = np.array([ train_x[i] for i in sample_ids])\n",
    "            train_y_shuff = np.array([ train_y[i] for i in sample_ids])\n",
    "            \n",
    "            \n",
    "            # Begin Annealing\n",
    "            if beginAnneal == 0:\n",
    "                lr = lr * 1.0\n",
    "            elif epoch+1 > beginAnneal:\n",
    "                lr = max( lr_min, (-decay_rate*(epoch+1) + (1+decay_rate*beginAnneal)) * lr )  \n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            # Calculate how many mini-batch iterations we need\n",
    "            total_batch = int(np.shape(train_x)[0]/batch_size) \n",
    "            \n",
    "            cost_epoch=0.0\n",
    "            \n",
    "            # minibatch based training  \n",
    "            for batch in np.arange(total_batch):\n",
    "                batch_x = train_x_shuff[batch*batch_size:(batch+1)*batch_size]\n",
    "                batch_y = train_y_shuff[batch*batch_size:(batch+1)*batch_size]\n",
    "                \n",
    "                # Get cost and optimize the model\n",
    "                cost_batch,_=sess.run([cost,optimizer],{Lr:lr, X:batch_x, Y:batch_y, Beta:beta})\n",
    "\n",
    "                cost_epoch+=cost_batch/total_batch      \n",
    "        \n",
    "        \n",
    "                # weight sparsity control    \n",
    "                if mode=='layer':                   \n",
    "                    for i in np.arange(np.shape(nodes)[0]-2):\n",
    "                        [hsp_val[i], beta_val[i]] = Hoyers_sparsity_control(w[i], beta_val[i], max_beta[i], tg_hspset[i])   \n",
    "                    beta=beta_val                      \n",
    "\n",
    "                elif mode=='node':                             \n",
    "                    for i in np.arange(np.shape(nodes)[0]-2):\n",
    "                        [hsp_val[i], beta_val[i]] = Hoyers_sparsity_control(w[i], beta_val[i], max_beta[i], tg_hspset[i])   \n",
    "                    # flatten beta_val (shape (3, 100) -> (300,))\n",
    "                    beta=[item for sublist in beta_val for item in sublist]\n",
    "               \n",
    "            # get train error\n",
    "            train_err_epoch=sess.run(error,{X:train_x_shuff, Y:train_y_shuff})\n",
    "            plot_train_err=np.hstack([plot_train_err,[train_err_epoch]])\n",
    "            \n",
    "            # get test error\n",
    "            test_err_epoch=sess.run(error,{X:test_x, Y:test_y})\n",
    "            plot_test_err=np.hstack([plot_test_err,[test_err_epoch]])\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Save the results to plot at the end\n",
    "            plot_lr=np.hstack([plot_lr,[lr]])\n",
    "            plot_cost=np.hstack([plot_cost,[cost_epoch]])\n",
    "            \n",
    "            if mode=='layer':\n",
    "                plot_hsp=[np.vstack([plot_hsp[i],[hsp_val[i]]]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "                plot_beta=[np.vstack([plot_beta[i],[beta[i]]]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "                \n",
    "            elif mode=='node':\n",
    "                plot_hsp=[np.vstack([plot_hsp[i],[np.transpose(hsp_val[i])]]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "                plot_beta=[np.vstack([plot_beta[i],[np.transpose(beta_val[i])]]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "\n",
    "            \n",
    "            # Print cost and errors after every training epoch       \n",
    "            print(\"< Epoch\", \"{:02d}\".format(epoch+1),\"> Cost :\", \"{:.3f}\".format(cost_epoch)\\\n",
    "                                            ,\"/ Train err :\", \"{:.3f}\".format(train_err_epoch),\"/ Test err :\",\"{:.3f}\".format(test_err_epoch))\n",
    "\n",
    "\n",
    "        # Print final accuracy on test set\n",
    "        print (\"\")\n",
    "        print(\"* Test accuracy :\", \"{:.3f}\".format(1-sess.run(error,{X:test_x, Y:test_y})))\n",
    "            \n",
    "else:\n",
    "    # Don't run the session but print 'failed' if any condition is not met\n",
    "    print(\"Failed!\")  \n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07. Plot & save results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, show the the results and save them as .mat file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "if condition==True:\n",
    "       \n",
    "    # Plot the change of learning rate\n",
    "    plt.title(\"Learning rate plot\",fontsize=16)\n",
    "    plot_lr=plot_lr[1:]\n",
    "    plt.ylim(0.0, lr_init*1.2)\n",
    "    plt.plot(plot_lr)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the change of cost\n",
    "    plt.title(\"Cost plot\",fontsize=16)\n",
    "    plot_cost=plot_cost[1:]\n",
    "    plt.plot(plot_cost)\n",
    "    plt.show()   \n",
    "    \n",
    " \n",
    "  \n",
    "    # Plot train & test error\n",
    "    plt.title(\"Train & Test error plot\",fontsize=16)\n",
    "    plot_train_err=plot_train_err[1:]\n",
    "    plt.plot(plot_train_err)\n",
    "    plt.hold\n",
    "    plot_test_err=plot_test_err[1:]\n",
    "    plt.plot(plot_test_err)\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    plt.legend(['Train error', 'Test error'],loc='upper right')\n",
    "    plt.show() \n",
    "\n",
    "\n",
    " \n",
    "    # Plot the change of beta value\n",
    "    print(\"\")       \n",
    "    for i in np.arange(np.shape(nodes)[0]-2):\n",
    "        print(\"\")\n",
    "        plt.title(\"Beta plot \\n Hidden layer %d\"%(i+1),fontsize=16)\n",
    "        plot_beta[i]=plot_beta[i][1:]\n",
    "        plt.plot(plot_beta[i])\n",
    "        plt.ylim(0.0, np.max(max_beta)*1.2)\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    # Plot the change of Hoyer's sparsity\n",
    "    print(\"\")            \n",
    "    for i in np.arange(np.shape(nodes)[0]-2):\n",
    "        print(\"\")\n",
    "        plt.title(\"Hoyer's sparsity plot \\n Hidden layer %d\"%(i+1),fontsize=16)\n",
    "        plot_hsp[i]=plot_hsp[i][1:]\n",
    "        plt.plot(plot_hsp[i])\n",
    "        plt.ylim(0.0, 1.0)\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    # make a new 'results' directory in the current directory\n",
    "    current_directory = os.getcwd()\n",
    "    final_directory = os.path.join(current_directory, r'results')\n",
    "    if not os.path.exists(final_directory):\n",
    "        os.makedirs(final_directory) \n",
    "        \n",
    "    # save results as .mat file\n",
    "    sio.savemat(final_directory+\"/result_learningrate.mat\", mdict={'lr': plot_lr})\n",
    "    sio.savemat(final_directory+\"/result_cost.mat\", mdict={'cost': plot_cost})\n",
    "    sio.savemat(final_directory+\"/result_train_err.mat\", mdict={'trainErr': plot_train_err})\n",
    "    sio.savemat(final_directory+\"/result_test_err.mat\", mdict={'testErr': plot_test_err})\n",
    "    sio.savemat(final_directory+\"/result_beta.mat\", mdict={'beta': plot_beta})\n",
    "    sio.savemat(final_directory+\"/result_hsp.mat\", mdict={'hsp': plot_hsp})\n",
    "\n",
    "else:\n",
    "    None "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
