{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight sparsity control (Tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is written for Python 3.5/3.6 and Tensorflow 1.1.0.  \n",
    "This code is for weight sparsity control in MLP. You may easily implement weight sparsity control follwing several steps described below.  \n",
    "\n",
    "00. Import  \n",
    "    - Importing modules \n",
    "01. Parameters\n",
    "    - 'mode, optimizer_algorithm, nodes, total_epoch, batch_size, beginAnneal, decay_rate, lr_init, min_lr,lr_beta, L2_param, max_beta, tg_hsp' on GUI \n",
    "02. Input data\n",
    "    - Train and test data set from sample data set\n",
    "03. Building model\n",
    "    - Building model with L1 and L2 regulariation term using trainig data and test model using test data\n",
    "04. Variables declaration\n",
    "    - beta(L1 reg parameter), L1 & L2 regularzation, cost, optimizer, other variables, and Hoyer's sparsity control function.\n",
    "05. Condition check\n",
    "    - Check the conditions to catch some possible errors.\n",
    "06. Learning\n",
    "    - Train the model\n",
    "07. Plot & save results\n",
    "    - plot the results and save them as .mat file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00. Import "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, call modules which containing Python definitions and statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This import statement gives Python access to all of TensorFlow's classes, methods, and symbols. \n",
    "import tensorflow as tf\n",
    "# The fundamental package for scientific computing with Python.\n",
    "import numpy as np\n",
    "# Linear algebra module for calculating L1 and L2 norm  \n",
    "from numpy import linalg as LA\n",
    "# To plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "# To check the directory when saving the results\n",
    "import os.path\n",
    "# The module for file input and output\n",
    "import scipy.io as sio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 01. Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can specify either using weight sparsity control mode in layer wise or node wise, and also may select optimzer algorithm among five selections.  \n",
    "Then set the number of nodes the value of first element is for input layer, the last one for output layer,  and the others in the middle for hidden layers.  \n",
    "We can adjust learning rate parameters total epoch, mini-batch size, when to begin learning rate annealing, decaying rate of learning rate, initial value of learning rate, and minimum value of learning rate. Besides, learning rate of beta of weight sparsity control(for L1 regularization) and L2 regularization parameter(for L2 regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Select optimizer\n",
    "'GradientDescent'\n",
    "'Adagrad'\n",
    "'Adam'\n",
    "'Momentum'\n",
    "'RMSProp'\n",
    "\"\"\"\n",
    "optimizer_algorithm='GradientDescent'\n",
    "\n",
    "momentum=0.01\n",
    "\n",
    "\"\"\" \n",
    "Set the number of nodes for input, output and each hidden layer here\n",
    "\"\"\"\n",
    "nodes=[74484,100,100,100,4]\n",
    "\n",
    "\"\"\"\n",
    "Set learning parameters\n",
    "\"\"\"\n",
    "\n",
    "# Set total epoch\n",
    "n_epochs=300\n",
    "# Set mini batch size\n",
    "batch_size=40\n",
    "# Let anealing to begin after **th epoch\n",
    "beginAnneal=50\n",
    "# anealing decay rate\n",
    "decay_rate=0.0005\n",
    "# Set initial learning rate and minimum                     \n",
    "lr_init = 1e-3    \n",
    "lr_min = 1e-4\n",
    "\n",
    "# Set learning rate of beta for weight sparsity control\n",
    "beta_lrates = 1e-2\n",
    "# Set L2 parameter for L2 regularization\n",
    "L2_reg= 1e-4\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Select the sparsity control mode\n",
    "'layer' for layer wise sparsity control\n",
    "'node' for node wise sparsity control\n",
    "\"\"\"\n",
    "mode = 'node'\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Set maximum beta value of each hidden layer\n",
    "and set target sparsness value (0:dense~1:sparse)\n",
    "\"\"\"\n",
    "# layer wise\n",
    "max_beta = [0.05, 0.95, 0.7]\n",
    "tg_hspset = [0.7, 0.7, 0.5]\n",
    "\n",
    "## node_wise\n",
    "#max_beta = [0.05, 0.8, 0.8]\n",
    "#tg_hspset = [0.7, 0.5, 0.5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset we provides consists of train, test, and validation parts. We will use train and test parts. Then here we split dataset into train_x, train_y, test_x and test_y.  \n",
    "Input dimension is 74484, and the number of output nodes for classification is four (Left-hand clecnhing (LH), right-hand clecnhing, auditory attention (AD), and visual stimulus (VS) tasks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "datasets = sio.loadmat('lhrhadvs_sample_data.mat')\n",
    "\n",
    "################ lhrhadvs_sample_data.mat ##################\n",
    "# train_x  = 240 volumes x 74484 voxels  \n",
    "# train_y  = 240 volumes x 1 [0:LH, 1:RH, 2:AD, 3:VS]\n",
    "# test_x  = 120 volumes x 74484 voxels\n",
    "# test_y  = 120 volumes x 1 [0:LH, 1:RH, 2:AD, 3:VS]\n",
    "############################################################\n",
    "\n",
    "\n",
    "train_x = datasets['train_x']\n",
    "train_y = np.zeros((np.shape(datasets['train_y'])[0],np.max(datasets['train_y'])+1))\n",
    "# transform into One-hot\n",
    "for i in np.arange(np.shape(datasets['train_y'])[0]):\n",
    "    train_y[i][datasets['train_y'][i][0]]=1 \n",
    "\n",
    "\n",
    "test_x = datasets['test_x']\n",
    "test_y = np.zeros((np.shape(datasets['test_y'])[0],np.max(datasets['test_y'])+1))\n",
    "# transform into One-hot\n",
    "for i in np.arange(np.shape(datasets['test_y'])[0]):\n",
    "    test_y[i][datasets['test_y'][i][0]]=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. Building model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part builds the MLP model by concatenating all layers based on the information we provided earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 'node_index' to split placeholder, for an example, given hidden_nodes=[100, 100, 100], nodes_index=[0, 100, 200, 300]\n",
    "nodes_index= [int(np.sum(nodes[1:i+1])) for i in np.arange(np.shape(nodes)[0]-1)]\n",
    "\n",
    "# Make two placeholders to fill the values later when training or testing\n",
    "X=tf.placeholder(tf.float32,[None,nodes[0]])\n",
    "Y=tf.placeholder(tf.float32,[None,nodes[-1]])\n",
    "\n",
    "# Create randomly initialized weight variables \n",
    "w_init=[tf.div(tf.random_normal([nodes[i],nodes[i+1]]), tf.sqrt(float(nodes[i])/2)) for i in np.arange(np.shape(nodes)[0]-1)]\n",
    "w=[tf.Variable(w_init[i], dtype=tf.float32) for i in np.arange(np.shape(nodes)[0]-1)]\n",
    "# Create randomly initialized bias variables \n",
    "b=[tf.Variable(tf.random_normal([nodes[i+1]]), dtype=tf.float32) for i in np.arange(np.shape(nodes)[0]-1)]\n",
    "\n",
    "# Build MLP model \n",
    "hidden_layers=[0.0]*(np.shape(nodes)[0]-2)\n",
    "for i in np.arange(np.shape(nodes)[0]-2):\n",
    "    # Input layer\n",
    "    if i==0:\n",
    "        hidden_layers[i]=tf.add(tf.matmul(X,w[i]),b[i])\n",
    "        hidden_layers[i]=tf.nn.tanh(hidden_layers[i])\n",
    "    # The other layers    \n",
    "    else:     \n",
    "        hidden_layers[i]=tf.add(tf.matmul(hidden_layers[i-1],w[i]),b[i])\n",
    "        hidden_layers[i]=tf.nn.tanh(hidden_layers[i])\n",
    "# Output layer\n",
    "output_layer=tf.add(tf.matmul(hidden_layers[-1],w[-1]),b[-1])\n",
    "\n",
    "# Logistic regression layer\n",
    "logRegression_layer=tf.nn.tanh(output_layer)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. Variables declaration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitions of functions to create and initialize\n",
    "- beta (L1 reg parameter)\n",
    "- L1 regularzation\n",
    "- L2 regularzation\n",
    "- cost\n",
    "- optimizer : GradientDescent, Adagrad, Adam... \n",
    "- other variables : lr, beta_val, beta, hsp_val, plot_beta...\n",
    "- sparsity control function : layer wise of node wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Make placeholders for total beta array (make a long one to concatenate every beta vector) \n",
    "def init_beta():\n",
    "    if mode=='layer':\n",
    "        # The size is same with the number of layers\n",
    "        Beta=tf.placeholder(tf.float32,[np.shape(nodes)[0]-2])\n",
    "    elif mode=='node':\n",
    "        # The size is same with the number of nodes\n",
    "        Beta=tf.placeholder(tf.float32,[np.sum(nodes[1:-1])])\n",
    "\n",
    "    return Beta\n",
    "\n",
    "\n",
    "# Make L1 loss term for regularization\n",
    "def init_L1():\n",
    "    if mode=='layer':\n",
    "        # Get L1 loss term by simply multiplying beta(scalar value) and L1 norm of weight for each layer\n",
    "        L1=[Beta[i]*tf.reduce_sum(abs(w[i])) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "    elif mode=='node':\n",
    "        # Get L1 loss term by multiplying beta(vector values as many as nodes) and L1 norm of weight for each layer\n",
    "        L1=[tf.reduce_sum(tf.matmul(abs(w[i]),tf.cast(tf.diag(Beta[nodes_index[i]:nodes_index[i+1]]),tf.float32))) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "        \n",
    "    return L1\n",
    "\n",
    "\n",
    "# Make L2 loss term for regularization\n",
    "def init_L2():\n",
    "    L2=[tf.reduce_sum(tf.square(w[i])) for i in np.arange(np.shape(nodes)[0]-1)] \n",
    "    \n",
    "    \n",
    "    return L2\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "# Define cost term (Cost = cross entropy + L1 term + L2 term )    \n",
    "def init_cost():\n",
    "\n",
    "    # A softmax regression : it adds up the evidence of our input being in certain classes, and converts that evidence into probabilities.\n",
    "    cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logRegression_layer, labels=Y)) \\\n",
    "                                     + tf.reduce_sum(L1) + L2_reg*tf.reduce_sum(L2) \n",
    "    \n",
    "    return cost\n",
    "\n",
    "\n",
    "# TensorFlow provides optimizers that slowly change each variable in order to minimize the loss function.\n",
    "def init_optimizer(Lr):\n",
    "    if optimizer_algorithm=='GradientDescent':\n",
    "        optimizer=tf.train.GradientDescentOptimizer(Lr).minimize(cost) \n",
    "    elif optimizer_algorithm=='Adagrad':\n",
    "        optimizer=tf.train.AdagradOptimizer(Lr).minimize(cost) \n",
    "    elif optimizer_algorithm=='Adam':\n",
    "        optimizer=tf.train.AdamOptimizer(Lr).minimize(cost) \n",
    "    elif optimizer_algorithm=='Momentum':\n",
    "        optimizer=tf.train.MomentumOptimizer(Lr).minimize(cost) \n",
    "    elif optimizer_algorithm=='RMSProp':\n",
    "        optimizer=tf.train.RMSPropOptimizer(Lr).minimize(cost) \n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "\n",
    "# initialization   \n",
    "def init_otherVariables():           \n",
    "    if mode=='layer': \n",
    "        beta_val = np.zeros(np.shape(nodes)[0]-2)\n",
    "        beta = np.zeros(np.shape(nodes)[0]-2)\n",
    "        hsp_val = np.zeros(np.shape(nodes)[0]-2)            \n",
    "        result_beta = np.zeros(np.shape(nodes)[0]-2)\n",
    "        result_hsp = np.zeros(np.shape(nodes)[0]-2)\n",
    "                   \n",
    "    elif mode=='node':                       \n",
    "        beta_val = [np.zeros(nodes[i+1]) for i in np.arange(np.shape(nodes)[0]-2)]  \n",
    "        beta = np.zeros(np.sum(nodes[1:-1]))\n",
    "        hsp_val = [np.zeros(nodes[i+1]) for i in np.arange(np.shape(nodes)[0]-2)]            \n",
    "        result_beta = [np.zeros(nodes[i+1]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "        result_hsp = [np.zeros(nodes[i+1]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "    \n",
    "    # make arrays to store and plot results\n",
    "    result_lr=np.zeros(1)\n",
    "    result_cost=np.zeros(1)\n",
    "    result_train_err=np.zeros(1)\n",
    "    result_test_err=np.zeros(1)\n",
    "      \n",
    "    \n",
    "    return beta_val, beta, hsp_val, result_beta, result_hsp, result_lr, result_cost, result_train_err, result_test_err\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create them by using those functions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# initialize learning rate\n",
    "lr = lr_init \n",
    "    \n",
    "# Make a placeholder to be able to update learning rate (Learning rate decaying) \n",
    "Lr=tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "Beta = init_beta()\n",
    "L1 = init_L1()\n",
    "L2 = init_L2()\n",
    "cost = init_cost()\n",
    "\n",
    "optimizer=init_optimizer(Lr)\n",
    "\n",
    " \n",
    "correct_prediction=tf.equal(tf.argmax(logRegression_layer,1),tf.argmax(Y,1))  \n",
    "# calculate an average error depending on how frequent it classified correctly   \n",
    "error=1-tf.reduce_mean(tf.cast(correct_prediction,tf.float32))      \n",
    "\n",
    "\n",
    "beta_val, beta, hsp_val, result_beta, result_hsp, result_lr, result_cost, result_train_err, result_test_err = init_otherVariables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if mode=='layer':\n",
    "    # Weight sparsity control with Hoyer's sparsness (Layer wise)  \n",
    "    def Hoyers_sparsity_control(w_,b,max_b,tg):\n",
    "        \n",
    "        # Get value of weight\n",
    "        W=sess.run(w_)\n",
    "        [nodes,dim]=W.shape  \n",
    "        \n",
    "        # vectorize weight matrix \n",
    "        Wvec=W.flatten()     \n",
    "        sqrt_nsamps=np.sqrt(Wvec.shape[0])\n",
    "        \n",
    "        # Calculate L1 and L2 norm     \n",
    "        L1norm=LA.norm(Wvec,1)\n",
    "        L2norm=LA.norm(Wvec,2)\n",
    "        \n",
    "        # Calculate hoyer's sparsness\n",
    "        h=(sqrt_nsamps-(L1norm/L2norm))/(sqrt_nsamps-1)\n",
    "        \n",
    "        # Update beta\n",
    "        b-=beta_lrates*np.sign(h-tg)\n",
    "        \n",
    "        # Trim value\n",
    "        b=0.0 if b<0.0 else b\n",
    "        b=max_b if b>max_b else b\n",
    "                         \n",
    "        return [h,b]\n",
    "    \n",
    "    \n",
    "elif mode=='node':   \n",
    "    # Weight sparsity control with Hoyer's sparsness (Node wise)\n",
    "    def Hoyers_sparsity_control(w_,b_vec,max_b,tg):\n",
    "    \n",
    "        # Get value of weight\n",
    "        W=sess.run(w_)\n",
    "        [nodes,dim]=W.shape\n",
    "        sqrt_nsamps=np.sqrt(nodes)\n",
    "        \n",
    "        # Calculate L1 and L2 norm \n",
    "        L1norm=LA.norm(W,1,axis=0)\n",
    "        L2norm=LA.norm(W,2,axis=0)\n",
    "     \n",
    "        # Calculate hoyer's sparsness\n",
    "        h_vec = np.zeros((1,dim))\n",
    "        h_vec=(sqrt_nsamps-(L1norm/L2norm))/(sqrt_nsamps-1)\n",
    "        \n",
    "        tg_vec = np.ones(dim)*tg\n",
    "        # Update beta       \n",
    "        b_vec-=beta_lrates*np.sign(h_vec-tg_vec)\n",
    "        \n",
    "        # Trim value\n",
    "        b_vec[b_vec<0.0]=0.0\n",
    "        b_vec[b_vec>max_b]=max_b\n",
    "        \n",
    "               \n",
    "        return [h_vec,b_vec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05. Condition check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the conditions to catch some possible errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "condition=False\n",
    "\n",
    "print()\n",
    "\n",
    "if np.size(nodes) <3:\n",
    "    print(\"Error : The number of total layers is not enough.\")\n",
    "elif (np.size(nodes)-2) != np.size(max_beta):\n",
    "    print(\"Error : The number of hidden layers and max beta values don't match. \")\n",
    "elif (np.size(nodes)-2) != np.size(tg_hspset):\n",
    "    print(\"Error : The number of hidden layers and target sparsity values don't match.\")\n",
    "elif np.size(train_x,axis=0) != np.size(train_y,axis=0):\n",
    "    print(\"Error : The sizes of input train datasets and output train datasets don't match. \")  \n",
    "elif np.size(test_x,axis=0) != np.size(test_y,axis=0):\n",
    "    print(\"Error : The sizes of input test datasets and output test datasets don't match. \")     \n",
    "elif (np.any(np.array(tg_hspset)<0)) | (np.any(np.array(tg_hspset)>1)):  \n",
    "    print(\"Error : The values of target sparsities are inappropriate.\")\n",
    "else:\n",
    "    condition=True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06. Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If condition is satisfied, we may start session where training and tesing proceed.\n",
    "Firstly, start learning and get cost and optimize, for all epochs. In every epoch, training data is split into mini batches so that every learning iteration is mini batch based learning.\n",
    "At the end of every epoch, get training error and test error, as well as, save cost, learning rate, beta, hsp and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Epoch 01 > Cost : 817.439 / Train err : 0.738 / Test err : 0.667\n",
      "             beta : [ 0.025  0.03   0.03 ]\n",
      "             hsp : [ 0.10627539  0.10128214  0.10358864]\n",
      "< Epoch 02 > Cost : 1632.795 / Train err : 0.717 / Test err : 0.667\n",
      "             beta : [ 0.03333333  0.06        0.06      ]\n",
      "             hsp : [ 0.14870531  0.13548081  0.13856398]\n",
      "< Epoch 03 > Cost : 1661.832 / Train err : 0.712 / Test err : 0.658\n",
      "             beta : [ 0.0375  0.09    0.09  ]\n",
      "             hsp : [ 0.17521497  0.15314135  0.15662085]\n",
      "< Epoch 04 > Cost : 1692.979 / Train err : 0.704 / Test err : 0.650\n",
      "             beta : [ 0.04  0.12  0.12]\n",
      "             hsp : [ 0.19538412  0.16437275  0.16809749]\n",
      "< Epoch 05 > Cost : 1724.945 / Train err : 0.700 / Test err : 0.642\n",
      "             beta : [ 0.04166667  0.15        0.15      ]\n",
      "             hsp : [ 0.21239923  0.17254533  0.17644067]\n",
      "< Epoch 06 > Cost : 1756.487 / Train err : 0.696 / Test err : 0.625\n",
      "             beta : [ 0.04285714  0.18        0.18      ]\n",
      "             hsp : [ 0.22761976  0.17910358  0.18312836]\n",
      "< Epoch 07 > Cost : 1786.393 / Train err : 0.688 / Test err : 0.608\n",
      "             beta : [ 0.04375  0.21     0.21   ]\n",
      "             hsp : [ 0.24172031  0.18477266  0.18889962]\n",
      "< Epoch 08 > Cost : 1813.512 / Train err : 0.683 / Test err : 0.600\n",
      "             beta : [ 0.04444444  0.24        0.24      ]\n",
      "             hsp : [ 0.25507197  0.18995197  0.19416613]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7c84d8ee93d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# get train error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mtrain_err_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_x_shuff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_y_shuff\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0mresult_train_err\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresult_train_err\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_err_epoch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hailey/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hailey/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hailey/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/hailey/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hailey/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "if condition==True:\n",
    "    \n",
    "\n",
    "    # variables are not initialized when you call tf.Variable\n",
    "    # To initialize all the variables in a TensorFlow program, you must explicitly call a special operation         \n",
    "    init = tf.global_variables_initializer()              \n",
    "     \n",
    "\n",
    "    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:           \n",
    "    \n",
    "        # run tensorflow variable initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "\n",
    "        # Start training \n",
    "        for epoch in np.arange(n_epochs):            \n",
    "                   \n",
    "            # Shuffle training data at the begining of each epoch           \n",
    "            total_sample = np.size(train_x, axis=0)\n",
    "            sample_ids = np.arange(total_sample)\n",
    "            np.random.shuffle(sample_ids) \n",
    "            \n",
    "            train_x_shuff = np.array([ train_x[i] for i in sample_ids])\n",
    "            train_y_shuff = np.array([ train_y[i] for i in sample_ids])\n",
    "            \n",
    "            \n",
    "            # Begin Annealing\n",
    "            if beginAnneal == 0:\n",
    "                lr = lr * 1.0\n",
    "            elif epoch+1 > beginAnneal:\n",
    "                lr = max( lr_min, (-decay_rate*(epoch+1) + (1+decay_rate*beginAnneal)) * lr )  \n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            # Calculate how many mini-batch iterations we need\n",
    "            total_batch = int(np.shape(train_x)[0]/batch_size) \n",
    "            \n",
    "            cost_epoch=0.0\n",
    "            \n",
    "            # minibatch based training  \n",
    "            for batch in np.arange(total_batch):\n",
    "                batch_x = train_x_shuff[batch*batch_size:(batch+1)*batch_size]\n",
    "                batch_y = train_y_shuff[batch*batch_size:(batch+1)*batch_size]\n",
    "                \n",
    "                # Get cost and optimize the model\n",
    "                cost_batch,_=sess.run([cost,optimizer],{Lr:lr, X:batch_x, Y:batch_y, Beta:beta})\n",
    "\n",
    "                cost_epoch+=cost_batch/total_batch      \n",
    "        \n",
    "        \n",
    "                # weight sparsity control    \n",
    "                if mode=='layer':                   \n",
    "                    for i in np.arange(np.shape(nodes)[0]-2):\n",
    "                        [hsp_val[i], beta_val[i]] = Hoyers_sparsity_control(w[i], beta_val[i], max_beta[i], tg_hspset[i])   \n",
    "                    beta=beta_val                      \n",
    "\n",
    "                elif mode=='node':                             \n",
    "                    for i in np.arange(np.shape(nodes)[0]-2):\n",
    "                        [hsp_val[i], beta_val[i]] = Hoyers_sparsity_control(w[i], beta_val[i], max_beta[i], tg_hspset[i])   \n",
    "                    # flatten beta_val (shape (3, 100) -> (300,))\n",
    "                    beta=[item for sublist in beta_val for item in sublist]\n",
    "               \n",
    "            # get train error\n",
    "            train_err_epoch=sess.run(error,{X:train_x_shuff, Y:train_y_shuff})\n",
    "            result_train_err=np.hstack([result_train_err,[train_err_epoch]])\n",
    "            \n",
    "            # get test error\n",
    "            test_err_epoch=sess.run(error,{X:test_x, Y:test_y})\n",
    "            result_test_err=np.hstack([result_test_err,[test_err_epoch]])\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Save the results to plot at the end\n",
    "            result_lr=np.hstack([result_lr,[lr]])\n",
    "            result_cost=np.hstack([result_cost,[cost_epoch]])\n",
    "            \n",
    "            if mode=='layer':\n",
    "                result_hsp=[np.vstack([result_hsp[i],[hsp_val[i]]]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "                result_beta=[np.vstack([result_beta[i],[beta[i]]]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "                \n",
    "            elif mode=='node':\n",
    "                result_hsp=[np.vstack([result_hsp[i],[np.transpose(hsp_val[i])]]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "                result_beta=[np.vstack([result_beta[i],[np.transpose(beta_val[i])]]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "\n",
    "            \n",
    "            # Print cost and errors after every training epoch       \n",
    "            print(\"< Epoch\", \"{:02d}\".format(epoch+1),\"> Cost :\", \"{:.3f}\".format(cost_epoch)\\\n",
    "                                            ,\"/ Train err :\", \"{:.3f}\".format(train_err_epoch),\"/ Test err :\",\"{:.3f}\".format(test_err_epoch)) \n",
    "            print(\"             beta :\",np.mean(np.mean(result_beta,axis=1),axis=1))\n",
    "            print(\"             hsp :\",np.mean(np.mean(result_hsp,axis=1),axis=1))  \n",
    "\n",
    "        # Print final accuracy on test set\n",
    "        print(\"\")\n",
    "        print(\"* Test accuracy :\", \"{:.3f}\".format(1-sess.run(error,{X:test_x, Y:test_y})))\n",
    "            \n",
    "else:\n",
    "    # Don't run the session but print 'failed' if any condition is not met\n",
    "    print(\"Failed!\")  \n",
    "     \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07. Plot & save results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, show the the results and save them as .mat file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if condition==True:\n",
    "    \n",
    "    \n",
    "    # make a new 'results' directory in the current directory\n",
    "    current_directory = os.getcwd()\n",
    "    final_directory = os.path.join(current_directory, r'results')\n",
    "    if not os.path.exists(final_directory):\n",
    "        os.makedirs(final_directory) \n",
    "       \n",
    "    f = open(final_directory+\"/parameters.txt\",'w') #opens file with name of \"test.txt\"\n",
    "    f.write('mode : '+str(mode)+'\\n')\n",
    "    f.write('optimizer_algorithm : '+str(optimizer_algorithm)+'\\n')\n",
    "    f.write('n_epochs : '+str(n_epochs)+'\\n')\n",
    "    f.write('batch_size : '+str(batch_size)+'\\n')\n",
    "    f.write('beginAnneal : '+str(beginAnneal)+'\\n')\n",
    "    f.write('decay_rate : '+str(decay_rate)+'\\n')\n",
    "    f.write('lr_init : '+str(lr_init)+'\\n')\n",
    "    f.write('lr_min : '+str(lr_min)+'\\n')\n",
    "    f.write('beta_lrates : '+str(beta_lrates)+'\\n')\n",
    "    f.write('L2_reg : '+str(L2_reg)+'\\n')\n",
    "    f.write('max_beta : '+str(max_beta)+'\\n')\n",
    "    f.write('tg_hspset : '+str(max_beta)+'\\n')\n",
    "    f.close()\n",
    "\n",
    "      \n",
    "    # Plot the change of learning rate\n",
    "    plt.figure() \n",
    "    plt.title(\"Learning rate plot\",fontsize=16)\n",
    "    result_lr=result_lr[1:]\n",
    "    plt.ylim(0.0, lr_init*1.2)\n",
    "    plt.plot(result_lr)\n",
    "    plt.savefig(final_directory+'/learning_rate.png')\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    \n",
    "    # Plot the change of cost\n",
    "    plt.figure() \n",
    "    plt.title(\"Cost plot\",fontsize=16)\n",
    "    result_cost=result_cost[1:]\n",
    "    plt.plot(result_cost)\n",
    "    plt.savefig(final_directory+'/cost.png')\n",
    "    plt.show(block=False)   \n",
    "    \n",
    " \n",
    "  \n",
    "    # Plot train & test error\n",
    "    plt.figure() \n",
    "    plt.title(\"Train & Test error plot\",fontsize=16)\n",
    "    result_train_err=result_train_err[1:]\n",
    "    plt.plot(result_train_err)\n",
    "    plt.hold\n",
    "    result_test_err=result_test_err[1:]\n",
    "    plt.plot(result_test_err)\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    plt.legend(['Train error', 'Test error'],loc='upper right')\n",
    "    plt.savefig(final_directory+'/error.png')\n",
    "    plt.show(block=False) \n",
    "    \n",
    "\n",
    "\n",
    " \n",
    "    # Plot the change of beta value\n",
    "    print(\"\")       \n",
    "    plt.figure() \n",
    "    for i in np.arange(np.shape(nodes)[0]-2):\n",
    "        print(\"\")\n",
    "        plt.title(\"Beta plot \\n Hidden layer %d\"%(i+1),fontsize=16)\n",
    "        result_beta[i]=result_beta[i][1:]\n",
    "        plt.plot(result_beta[i])\n",
    "        plt.ylim(0.0, np.max(max_beta)*1.2)\n",
    "        plt.savefig(final_directory+'/beta%d.png'%(i+1))\n",
    "        plt.show(block=False)\n",
    "    \n",
    "    \n",
    "    # Plot the change of Hoyer's sparsity\n",
    "    print(\"\")      \n",
    "    plt.figure()       \n",
    "    for i in np.arange(np.shape(nodes)[0]-2):\n",
    "        print(\"\")\n",
    "        plt.title(\"Hoyer's sparsity plot \\n Hidden layer %d\"%(i+1),fontsize=16)\n",
    "        result_hsp[i]=result_hsp[i][1:]\n",
    "        plt.plot(result_hsp[i])\n",
    "        plt.ylim(0.0, 1.0)\n",
    "        plt.savefig(final_directory+'/hsp%d.png'%(i+1))\n",
    "        plt.show(block=False)\n",
    "\n",
    "        \n",
    "    # save results as .mat file\n",
    "    sio.savemat(final_directory+\"/result_learningrate.mat\", mdict={'lr': result_lr})\n",
    "    sio.savemat(final_directory+\"/result_cost.mat\", mdict={'cost': result_cost})\n",
    "    sio.savemat(final_directory+\"/result_train_err.mat\", mdict={'trainErr': result_train_err})\n",
    "    sio.savemat(final_directory+\"/result_test_err.mat\", mdict={'testErr': result_test_err})\n",
    "    sio.savemat(final_directory+\"/result_beta.mat\", mdict={'beta': result_beta})\n",
    "    sio.savemat(final_directory+\"/result_hsp.mat\", mdict={'hsp': result_hsp})\n",
    "\n",
    "else:\n",
    "    None \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
