{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep neural network (DNN) with weight sparisity contorl via Hoyer sparseness\n",
    "\n",
    "DNN with weight sparsity control (i.e., L1-norm regularization) improved the classification performance using whole-brain resting-state functional connectivity patterns of schizophrenia patient and healthy groups (Jang et al., Neuroimage 2017, Kim et al., Neuroimage, 2016). The Python codes were modified from the DeepLearningTutorials (https://github.com/lisa-lab/DeepLearningTutorials) to apply the node-wise and layer-wise control of weight sparsity via Hoyer sparseness.\n",
    "\n",
    "== Structure of dnnwsp_hsp_theano.py ==\n",
    "00. Import\n",
    " * Importing necessary modules, functions \n",
    " \n",
    "01. Functiona definition\n",
    " * hsp_fnc, ReLU, RmsProp, adam\n",
    " \n",
    "02. Class definition\n",
    " * Hidden layer, multiple layer perceptron\n",
    " \n",
    "03. Parameters of dnnwsp\n",
    " * epoches, learning rates ...\n",
    " \n",
    "04. Input data\n",
    " * train and test data set from sample data set\n",
    " \n",
    "05. Build Model\n",
    " * Building training dnnwsp model with L1 and L2 regulariation term using trainig data and test model using test data\n",
    " \n",
    "06. Learning\n",
    " * Training dnnwsp model \n",
    " \n",
    "07. Save variables\n",
    "\n",
    " * Save important variables \n",
    "08. logistic_sgd.py\n",
    " * Logistic regression is a probabilistic, linear classifier \n",
    " \n",
    "09. Main function\n",
    " * Execution the dnnwsp code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 00. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from logistic_sgd import LogisticRegression\n",
    "\n",
    "try:\n",
    "    from StringIO import StringIO\n",
    "except ImportError:\n",
    "        from io import StringIO\n",
    "\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 01. Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the node-wise or layer-wise control of weight sparsity via Hoyer sparseness\n",
    "# (Hoyer, 2014, Kim and Lee PRNI2016, Kim and Lee ICASSP 2017)\n",
    "def hsp_fnc(beta_val_L1, W, max_beta, tg_hsp, beta_lrate,flag_nodewise):\n",
    "    W = np.array(W.get_value(borrow=True));\n",
    "    \n",
    "    cnt_L1_ly = beta_val_L1;\n",
    "    \n",
    "    if flag_nodewise==1:\n",
    "        \n",
    "        [dim, nodes] = W.shape # data dimensionaltiy x number of nodes \n",
    "        hsp_vec = np.zeros((1,nodes));  \n",
    "        tg_hsp_vec = np.ones(nodes)*tg_hsp;\n",
    "        sqrt_nsamps = pow(dim,0.5)\n",
    "        n1_W = LA.norm(W,1,axis=0);    n2_W = LA.norm(W,2,axis=0);\n",
    "        hsp_vec = (sqrt_nsamps - (n1_W/n2_W))/(sqrt_nsamps-1)\n",
    "        cnt_L1_ly -= beta_lrate*np.sign(hsp_vec-tg_hsp_vec)\n",
    "\n",
    "        for ii in range(0,nodes):\n",
    "            cnt_L1_ly[ii] = 0 if cnt_L1_ly[ii] < 0 else cnt_L1_ly[ii]\n",
    "            cnt_L1_ly[ii] = max_beta if cnt_L1_ly[ii]>max_beta else cnt_L1_ly[ii]\n",
    "            \n",
    "        return [hsp_vec, cnt_L1_ly]\n",
    "    else:\n",
    "        \n",
    "        Wvec = W.flatten(); # vectorize weight matrix \n",
    "        sqrt_nsamps = pow((Wvec.shape[0]),0.5)\n",
    "        n1_W = LA.norm(Wvec,1);    n2_W = LA.norm(Wvec,2);\n",
    "        hspvalue = (sqrt_nsamps-(n1_W/n2_W))/(sqrt_nsamps-1);\n",
    "        cnt_L1_ly -= beta_lrate*np.sign(hspvalue-tg_hsp) \n",
    "        \n",
    "        cnt_L1_ly = 0 if cnt_L1_ly<0 else cnt_L1_ly\n",
    "        cnt_L1_ly = max_beta if cnt_L1_ly>max_beta else cnt_L1_ly\n",
    "\n",
    "        return [hspvalue, cnt_L1_ly]\n",
    "\n",
    "# Define a rectified linear unit \n",
    "def relu1(x):\n",
    "    return T.switch(x<0, 0, x)\n",
    "\n",
    "# Define Root Mean Square Propagation (RMSprop) for the gradient descent optimization \n",
    "def RMSprop(cost, params, learning_rate, rho=0.9, epsilon=1e-6):\n",
    "    \n",
    "    updates = []\n",
    "    all_grads = [T.grad(cost, param) for param in params] # gradient estimate \n",
    "\n",
    "    for p, g in zip(params, all_grads):\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        gradient_scaling = T.sqrt(acc_new + epsilon)\n",
    "        g = g / gradient_scaling\n",
    "        updates.append((acc, acc_new))\n",
    "        updates.append((p, p - learning_rate * g))\n",
    "    return updates\n",
    "\n",
    "# Define Adaptive Moment Estimation for the gradient descent optimization(https://gist.github.com/skaae/ae7225263ca8806868cb)\n",
    "def adam(cost, params, learning_rate, b1=0.9, b2=0.999, e=1e-8, gamma=1-1e-8):\n",
    "    \n",
    "    \"\"\"\n",
    "    ADAM update rules\n",
    "    Default values are taken from [Kingma2014]\n",
    "    Reference: [Kingma2014] Kingma, Diederik, and Jimmy Ba. \"Adam: A Method for Stochastic Optimization.\"\n",
    "    arXiv preprint arXiv:1412.6980 (2014). http://arxiv.org/pdf/1412.6980v4.pdf\n",
    "    \"\"\"\n",
    "    \n",
    "    updates = []\n",
    "    all_grads = [T.grad(cost, param) for param in params] # gradient estimate \n",
    "\n",
    "    alpha = learning_rate\n",
    "    t = theano.shared(np.float32(1))\n",
    "    b1_t = b1*gamma**(t-1)   #(Decay the first moment running average coefficient)\n",
    "\n",
    "    for theta_previous, g in zip(params, all_grads):\n",
    "        m_previous = theano.shared(np.zeros(theta_previous.get_value().shape,\n",
    "                                            dtype=theano.config.floatX))\n",
    "        v_previous = theano.shared(np.zeros(theta_previous.get_value().shape,\n",
    "                                            dtype=theano.config.floatX))\n",
    "        m = b1_t*m_previous + (1 - b1_t)*g                             # (Update biased first moment estimate)\n",
    "        v = b2*v_previous + (1 - b2)*g**2                              # (Update biased second raw moment estimate)\n",
    "        m_hat = m / (1-b1**t)                                          # (Compute bias-corrected first moment estimate)\n",
    "        v_hat = v / (1-b2**t)                                          # (Compute bias-corrected second raw moment estimate)\n",
    "        theta = theta_previous - (alpha * m_hat) / (T.sqrt(v_hat) + e) #(Update parameters)\n",
    "\n",
    "        updates.append((m_previous, m))\n",
    "        updates.append((v_previous, v))\n",
    "        updates.append((theta_previous, theta) )\n",
    "    updates.append((t, t + 1.))\n",
    "    return updates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HiddenLayer(object):\n",
    "    \n",
    "    def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n",
    "                 activation=T.nnet.sigmoid):\n",
    "\n",
    "        self.input = input\n",
    "\n",
    "        if W is None:\n",
    "            W_values = numpy.asarray(\n",
    "                rng.uniform(\n",
    "                    low=-4*numpy.sqrt(6. / (n_in + n_out)),\n",
    "                    high=4*numpy.sqrt(6. / (n_in + n_out)),\n",
    "                    size=(n_in, n_out)\n",
    "                ),\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "        W = theano.shared(value=W_values, name='W', borrow=True)\n",
    "            \n",
    "        if b is None:\n",
    "            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n",
    "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
    "\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "\n",
    "        lin_output = T.dot(input, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if activation is None\n",
    "            else activation(lin_output)\n",
    "        )\n",
    "        # parameters of the model\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "        self.chk_pre_output = theano.shared(numpy.zeros((60,n_out), dtype=theano.config.floatX), name='chk_pre_output', borrow=True)\n",
    "        self.updates = [(self.chk_pre_output, lin_output)]\n",
    "\n",
    "class MLP(object):\n",
    "    \n",
    "    def __init__(self, rng, input, n_nodes, activation=T.nnet.sigmoid):\n",
    "\n",
    "        if len(n_nodes) > 2:\n",
    "            self.hiddenLayer = []\n",
    "            \n",
    "            for i in range(len(n_nodes)-2):\n",
    "                \n",
    "                if i == 0:\n",
    "                    hidden_input = input\n",
    "                else:\n",
    "                    hidden_input = self.hiddenLayer[i-1].output\n",
    "                    \n",
    "                self.hiddenLayer.append(\n",
    "                    HiddenLayer(\n",
    "                        rng=rng,\n",
    "                        input=hidden_input,\n",
    "                        n_in=n_nodes[i],\n",
    "                        n_out=n_nodes[i+1],\n",
    "                        W=None,\n",
    "                        b=None,\n",
    "                        activation=activation\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "        # The logistic regression layer gets as input the hidden units of the hidden layer\n",
    "        if len(n_nodes) == 2:\n",
    "            logistic_input = input\n",
    "        else:\n",
    "            logistic_input = self.hiddenLayer[len(n_nodes)-3].output\n",
    "                    \n",
    "        self.logRegressionLayer = LogisticRegression(\n",
    "            input=logistic_input,\n",
    "            n_in=n_nodes[len(n_nodes)-2],\n",
    "            n_out=n_nodes[len(n_nodes)-1]\n",
    "        )\n",
    "        \n",
    "        self.L1 = []\n",
    "        for i in range(len(n_nodes)-2):\n",
    "            self.L1.append(abs(self.hiddenLayer[i].W).sum() )\n",
    "        self.L1.append(abs(self.logRegressionLayer.W).sum() )    \n",
    "        \n",
    "        self.L2_sqr = 0\n",
    "        for i in range(len(n_nodes)-2):\n",
    "            self.L2_sqr += (self.hiddenLayer[i].W ** 2).sum()\n",
    "        self.L2_sqr += ((self.logRegressionLayer.W ** 2).sum())\n",
    "\n",
    "        # negative log likelihood of the MLP is given by the negative\n",
    "        # log likelihood of the output of the model, computed in the\n",
    "        # logistic regression layer\n",
    "        self.negative_log_likelihood = (\n",
    "            self.logRegressionLayer.negative_log_likelihood\n",
    "        )\n",
    "        # same holds for the function computing the number of errors\n",
    "        self.errors = self.logRegressionLayer.errors\n",
    "        self.mse = self.logRegressionLayer.mse\n",
    "        \n",
    "        self.params = []\n",
    "        if len(n_nodes) > 2:\n",
    "            for i in range(len(n_nodes)-2):\n",
    "                self.params.extend(self.hiddenLayer[i].params)\n",
    "        self.params.extend(self.logRegressionLayer.params)\n",
    "        \n",
    "        self.oldparams = [theano.shared(numpy.zeros(p.get_value(borrow=True).shape, dtype=theano.config.floatX)) for p in self.params]\n",
    "\n",
    "        # keep track of model input\n",
    "        self.input = input\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "       \n",
    "def test_mlp(n_nodes=[74484,100,100,100,4],  # input-hidden-nodees\n",
    "             datasets='lhrhadvs_sample_data.mat',  # load data\n",
    "             \n",
    "             # activation:  # sigmoid function: T.nnet.sigmoid, hyperbolic tangent function: T.tanh, Rectified Linear Unit: relu1\n",
    "             batch_size = 100, n_epochs = 500, learning_rate=0.001,activation = T.tanh,\n",
    "             beginAnneal=200, min_annel_lrate = 1e-4, decay_rate = 1e-4, momentum_val=0.00,\n",
    "             \n",
    "             # Select optimizer 'Grad' for GradientDescentOptimizer, 'Adam' for AdamOptimizer, 'Rmsp' for RMSPropOptimizer\n",
    "             optimizer_algorithm='Grad',\n",
    "                       \n",
    "             # Parameters for the node-wise control of weight sparsity\n",
    "             # if you have three hidden layer, the number of target Hoyer's sparseness should be same \n",
    "             tg_hspset=[0.7, 0.5, 0.5], # Target sparsity\n",
    "             max_beta=[0.05, 0.9, 0.9], # Maximum beta changes\n",
    "             \n",
    "             # Parameters for the layer-wise control of weight sparsity \n",
    "             # tg_hspset=[0.7, 0.5, 0.5], # Target sparsity \n",
    "             # max_beta=[0.05, 0.8, 0.8], # Maximum beta changes\n",
    "             beta_lrates = 1e-2,        L2_reg = 1e-5,\n",
    "             \n",
    "            # flag_nodewise =1 is the node-wise control of weight sparsity \n",
    "            # flag_nodewise =0 is the layer-wise control of weight sparsity\n",
    "            \n",
    "             flag_nodewise = 0,\n",
    "             # Save path  \n",
    "             sav_path = '/home/khc/workspace/prni2017',  \n",
    "              ):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "   datasets=sio.loadmat(datasets) # load datasets\n",
    "    \n",
    "    ############# lhrhadvs_sample_data.mat #############\n",
    "    # train_x  = 240 volumes x 74484 voxels  \n",
    "    # train_x  = 240 volumes x 1 [0:left-hand clenching task, 1:right-hand clenching task, 2:auditory task, 3:visual task]\n",
    "    # test_x  = 120 volumes x 74484 voxels\n",
    "    # test_y  = 120 volumes x 1 [0:left-hand clenching task, 1:right-hand clenching task, 2:auditory task, 3:visual task]\n",
    "    ############################################################\n",
    "\n",
    "    train_x = datasets['train_x']; \n",
    "    train_y = datasets['train_y'];\n",
    "    test_x  = datasets['test_x'];\n",
    "    test_y  = datasets['test_y'];\n",
    "    \n",
    "    train_set_x = theano.shared(numpy.asarray(train_x, dtype=theano.config.floatX))\n",
    "    train_set_y = T.cast(theano.shared(train_y.flatten(),borrow=True),'int32')\n",
    "    \n",
    "    test_set_x = theano.shared(numpy.asarray(test_x, dtype=theano.config.floatX))\n",
    "    test_set_y = T.cast(theano.shared(test_y.flatten(),borrow=True),'int32')\n",
    "\n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = int(train_set_x.get_value(borrow=True).shape[0] / batch_size)\n",
    "    n_test_batches = int(test_set_x.get_value(borrow=True).shape[0] / batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "   #####################\n",
    "    # BUILD ACTUAL MODEL #\n",
    "    ######################\n",
    "    print('... building the model')\n",
    "\n",
    "    # allocate symbolic variables for the data\n",
    "    index = T.lscalar()  # index to a [mini]batch\n",
    "    x = T.matrix('x')  \n",
    "    y = T.ivector('y')  # the labels are presented as 1D vector of [int] labels\n",
    "\n",
    "    l1_penalty_layer = T.fvector() #  L1-norm regularization parameter\n",
    "    ln_rate = T.scalar(name='learning_rate') # learning rate\n",
    "    momentum = T.scalar(name='momentum')\n",
    "                 \n",
    "    rng = numpy.random.RandomState(1234)\n",
    "\n",
    "    # construct the MLP class\n",
    "    classifier = MLP(\n",
    "        rng=rng,\n",
    "        input=x,\n",
    "        n_nodes = n_nodes,\n",
    "        activation = activation,\n",
    "    )\n",
    "\n",
    "    # cost function\n",
    "    cost = (classifier.negative_log_likelihood(y))\n",
    "    \n",
    "    if flag_nodewise==1:\n",
    "        for i in range(len(n_nodes)-2):\n",
    "            node_size = n_nodes[i+1]; tg_index = np.arange((i * node_size),((i + 1) * node_size));\n",
    "            cost += (T.dot(abs(classifier.hiddenLayer[i].W),l1_penalty_layer[tg_index])).sum();\n",
    "    else:\n",
    "        for i in range(len(n_nodes)-2):\n",
    "            cost += l1_penalty_layer[i] * classifier.L1[i]\n",
    "\n",
    "    cost += L2_reg * classifier.L2_sqr    \n",
    "\n",
    "    \n",
    "    updates_test = []\n",
    "    for hiddenlayer in classifier.hiddenLayer:\n",
    "        for i in range(1):\n",
    "            updates_test.append( hiddenlayer.updates[i] )\n",
    "           \n",
    "    test_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=[classifier.errors(y),classifier.mse(batch_size,n_nodes[-1],y)],\n",
    "        updates=updates_test,\n",
    "        givens={\n",
    "            x: test_set_x[index * batch_size:(index + 1) * batch_size],\n",
    "            y: test_set_y[index * batch_size:(index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    updates =[];\n",
    "    # Select optimizer 'Grad' for GradientDescentOptimizer, 'Adam' for AdamOptimizer, 'Rmsp' for RMSPropOptimizer\n",
    "    if optimizer_algorithm=='Grad':\n",
    "        gparams = [T.grad(cost, param) for param in classifier.params]\n",
    "        \n",
    "        for param, gparam, oldparam in zip(classifier.params, gparams, classifier.oldparams):\n",
    "            delta = ln_rate * gparam + momentum * oldparam\n",
    "            updates.append((param, param - delta))\n",
    "            updates.append((oldparam, delta))\n",
    "\n",
    "    elif optimizer_algorithm=='Adam':\n",
    "        updates = adam(cost, classifier.params, learning_rate)\n",
    "        \n",
    "    elif optimizer_algorithm=='Rmsp' :\n",
    "        updates = RMSprop(cost, classifier.params, learning_rate)\n",
    "    \n",
    "    # compiling a Theano function `train_model` that returns the cost, but\n",
    "    # in the same time updates the parameter of the model based on the rules\n",
    "    # defined in `updates`\n",
    "    train_model = theano.function(\n",
    "        inputs=[index, l1_penalty_layer,ln_rate,momentum],\n",
    "        outputs=[cost,classifier.errors(y),classifier.mse(batch_size,n_nodes[-1],y)],\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        },\n",
    "        allow_input_downcast = True,\n",
    "        on_unused_input = 'ignore'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    ###############\n",
    "    # TRAIN MODEL #\n",
    "    ###############\n",
    "    print('... training')\n",
    "\n",
    "    test_score = 0. \n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    epoch = 0;    done_looping = False\n",
    "    \n",
    "    # Define variables to save/check training model \n",
    "    train_errors = np.zeros(n_epochs);    test_errors = np.zeros(n_epochs);\n",
    "    train_mse = np.zeros(n_epochs);    test_mse = np.zeros(n_epochs);\n",
    "    lrs = np.zeros(n_epochs); lrate_list = np.zeros(n_epochs);\n",
    "    \n",
    "    if flag_nodewise==1:\n",
    "        hsp_avg_vals =[]; L1_beta_avg_vals=[];  all_hsp_vals =[]; all_L1_beta_vals=[];\n",
    "        L1_beta_vals = np.zeros(np.sum(n_nodes[1:(len(n_nodes)-1)]));\n",
    "        cnt_hsp_val = np.zeros(len(n_nodes)-2); \n",
    "        cnt_beta_val = np.zeros(len(n_nodes)-2);\n",
    "        \n",
    "        for i in range(len(n_nodes)-2):\n",
    "            hsp_avg_vals.append(np.zeros((n_epochs,n_nodes[i+1])));\n",
    "            L1_beta_avg_vals.append(np.zeros((n_epochs,n_nodes[i+1])));\n",
    "    \n",
    "            all_hsp_vals.append(np.zeros((n_epochs,n_nodes[i+1])));\n",
    "            all_L1_beta_vals.append(np.zeros((n_epochs,n_nodes[i+1])));\n",
    "        \n",
    "    else:\n",
    "        all_hsp_vals = np.zeros((n_epochs,len(n_nodes)-2));  \n",
    "        all_L1_beta_vals = np.zeros((n_epochs,len(n_nodes)-2));\n",
    "        \n",
    "        L1_beta_vals= np.zeros(len(n_nodes)-2)\n",
    "        cnt_hsp_val = np.zeros(len(n_nodes)-2);    \n",
    "\n",
    "    # start training \n",
    "    while (epoch < n_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        minibatch_all_avg_error = []; minibatch_all_avg_mse = []\n",
    "\n",
    "        # minibatch based training\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "            disply_text = StringIO();\n",
    "            minibatch_avg_cost, minibatch_avg_error, minibatch_avg_mse = train_model(minibatch_index, L1_beta_vals,learning_rate,momentum_val)\n",
    "            minibatch_all_avg_error.append(minibatch_avg_error)\n",
    "            minibatch_all_avg_mse.append(minibatch_avg_mse)\n",
    "             \n",
    "            # Node-wise or layer-wise control of weight sparsity \n",
    "            if flag_nodewise==1:\n",
    "                for i in range(len(n_nodes)-2):\n",
    "                    node_size = n_nodes[i+1]; tg_index = np.arange((i * node_size),((i + 1) * node_size));\n",
    "                    tmp_L1_beta_vals = L1_beta_vals[tg_index]\n",
    "                    [all_hsp_vals[i][epoch-1], L1_beta_vals[tg_index]] = hsp_fnc(tmp_L1_beta_vals,classifier.hiddenLayer[i].W,max_beta[i],tg_hspset[i],beta_lrates,flag_nodewise);\n",
    "                    all_L1_beta_vals[i][epoch-1]= L1_beta_vals[tg_index];\n",
    "            else:\n",
    "                for i in range(len(n_nodes)-2):\n",
    "                    [cnt_hsp_val[i], L1_beta_vals[i]] = hsp_fnc(L1_beta_vals[i],classifier.hiddenLayer[i].W,max_beta[i],tg_hspset[i],beta_lrates,flag_nodewise);\n",
    "                \n",
    "            # iteration number\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "            # test it on the test set\n",
    "            test_losses = []; test_mses = []\n",
    "            for i in range(n_test_batches):\n",
    "                test_losses.append(test_model(i)[0])\n",
    "                test_mses.append(test_model(i)[1])\n",
    "            test_score = numpy.mean(test_losses);\n",
    "             \n",
    "        # Begin Annealing\n",
    "        if beginAnneal == 0:\n",
    "            learning_rate = learning_rate * 1.0\n",
    "        elif epoch > beginAnneal:\n",
    "            learning_rate = max(min_annel_lrate, (-decay_rate*epoch + (1+decay_rate*beginAnneal)) * learning_rate )\n",
    "            \n",
    "        # Save variables to check training\n",
    "        train_errors[epoch-1] = np.mean(minibatch_all_avg_error)*100\n",
    "        test_errors[epoch-1] = test_score*100\n",
    "        train_mse[epoch-1] = np.mean(minibatch_all_avg_mse)\n",
    "        test_mse[epoch-1] = np.mean(test_mses)\n",
    "        \n",
    "        \n",
    "        # Node-wise or layer-wise control of weight sparsity to display the current state of training \n",
    "        if flag_nodewise ==1:\n",
    "            disply_text.write(\"Node-wise control, epoch %i/%d, Tr.err= %.2f, Ts.err= %.2f, lr = %.6f, \" % (epoch,n_epochs,train_errors[epoch-1],test_errors[epoch-1],learning_rate))\n",
    "\n",
    "            for layer_idx in range(len(n_nodes)-2):\n",
    "                cnt_hsp_val[layer_idx] = np.mean(all_hsp_vals[layer_idx][epoch-1])\n",
    "                cnt_beta_val[layer_idx] = np.mean(all_L1_beta_vals[layer_idx][epoch-1])\n",
    "                \n",
    "        else:  \n",
    "            disply_text.write(\"Layer-wise control, epoch %i/%d, Tr.err= %.2f, Ts.err= %.2f, lr = %.6f, \" % (epoch,n_epochs,train_errors[epoch-1],test_errors[epoch-1],learning_rate))\n",
    "            \n",
    "            all_hsp_vals[epoch-1,:] = cnt_hsp_val;\n",
    "            all_L1_beta_vals[epoch-1,:] = L1_beta_vals;\n",
    "            \n",
    "        for layer_idx in range(len(n_nodes)-2):\n",
    "            if (layer_idx==len(n_nodes)-3):\n",
    "                disply_text.write(\"hsp_l%d = %.2f/%.2f, beta_l%d = %.2f\" % (layer_idx+1,cnt_hsp_val[layer_idx],tg_hspset[layer_idx],layer_idx+1,L1_beta_vals[layer_idx]))\n",
    "            else:\n",
    "                disply_text.write(\"hsp_l%d = %.2f/%.2f, beta_l%d = %.2f, \" % (layer_idx+1,cnt_hsp_val[layer_idx],tg_hspset[layer_idx],layer_idx+1,L1_beta_vals[layer_idx]))\n",
    "                    \n",
    "        # Display variables                 \n",
    "        print(disply_text.getvalue())\n",
    "        disply_text.close()\n",
    "        \n",
    "        lrs[epoch-1] = learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07. Save variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    if not os.path.exists(sav_path): \n",
    "        os.makedirs(sav_path)\n",
    "        \n",
    "    end_time = timeit.default_timer()\n",
    "    cst_time = (end_time - start_time) / 60.\n",
    "    print(sys.stderr, ('\\n The code for file ' + os.path.split(__file__)[1] +\n",
    "                          ' ran for %.2fm' % ((end_time - start_time) / 60.)))\n",
    "     \n",
    "    sav_text = StringIO();\n",
    "    for layer_idx in range(len(n_nodes)-2):\n",
    "        if layer_idx==len(n_nodes)-3:\n",
    "            sav_text.write(\"%d\" % (n_nodes[layer_idx+1]))\n",
    "        else:\n",
    "            sav_text.write(\"%d-\" % (n_nodes[layer_idx+1]))\n",
    "\n",
    "    sav_name = '%s/mlp_rst_inv_%s.mat' % (sav_path,sav_text.getvalue())\n",
    "    sav_text.close()\n",
    "        \n",
    "    data_variable = {}; \n",
    "\n",
    "    for i in range(len(n_nodes)-1):\n",
    "        if (i==len(n_nodes)-2): \n",
    "            W_name = \"w%d\" %(i+1); b_name = \"b%d\" % (i+1); \n",
    "            data_variable[W_name] = classifier.logRegressionLayer.W.get_value(borrow=True)\n",
    "            data_variable[b_name] = classifier.logRegressionLayer.b.get_value(borrow=True)\n",
    "        else:\n",
    "            W_name = \"w%d\" %(i+1); b_name = \"b%d\" % (i+1)\n",
    "            data_variable[W_name] = classifier.hiddenLayer[i].W.get_value(borrow=True)\n",
    "            data_variable[b_name] = classifier.hiddenLayer[i].b.get_value(borrow=True)\n",
    "            \n",
    "    data_variable['hsp_vals'] = all_hsp_vals;  \n",
    "    data_variable['L1_vals'] =  all_L1_beta_vals;\n",
    "    data_variable['train_errors'] = train_errors;    data_variable['test_errors'] = test_errors;\n",
    "    data_variable['l_rate'] = lrs;\n",
    "    \n",
    "    data_variable['momtentum'] = momentum_val;    data_variable['beginAnneal'] = beginAnneal;    data_variable['decay_lr'] = decay_rate;\n",
    "    data_variable['beta_lrates'] = beta_lrates;    data_variable['max_beta'] = max_beta;    data_variable['tg_hspset'] = tg_hspset;\n",
    "    data_variable['batch_size'] = batch_size;    data_variable['n_epochs'] = n_epochs;    data_variable['min_annel_lrate'] = min_annel_lrate;\n",
    "    data_variable['n_nodes'] = n_nodes; data_variable['lrate_list'] = lrate_list;\n",
    "    \n",
    "    sio.savemat(sav_name,data_variable)\n",
    "\n",
    "    print('...done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08. logistic_sgd.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Logistic regression is a probabilistic, linear classifier. It is parametrized\n",
    "by a weight matrix :math:`W` and a bias vector :math:`b`. Classification is\n",
    "done by projecting data points onto a set of hyperplanes, the distance to\n",
    "which is used to determine a class membership probability.\n",
    "\n",
    "Mathematically, this can be written as:\n",
    "\n",
    ".. math::\n",
    "  P(Y=i|x, W,b) &= softmax_i(W x + b) \\\\\n",
    "                &= \\frac {e^{W_i x + b_i}} {\\sum_j e^{W_j x + b_j}}\n",
    "\n",
    "\n",
    "The output of the model or prediction is then done by taking the argmax of\n",
    "the vector whose i'th element is P(Y=i|x).\n",
    "\n",
    ".. math::\n",
    "\n",
    "  y_{pred} = argmax_i P(Y=i|x,W,b)\n",
    "\n",
    "\n",
    "This tutorial presents a stochastic gradient descent optimization method\n",
    "suitable for large datasets.\n",
    "\n",
    "\n",
    "References:\n",
    "\n",
    "    - textbooks: \"Pattern Recognition and Machine Learning\" -\n",
    "                 Christopher M. Bishop, section 4.3.2\n",
    "\n",
    "\"\"\"\n",
    "import numpy\n",
    "import scipy.io\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "class LogisticRegression(object):\n",
    "    \"\"\"Multi-class Logistic Regression Class\n",
    "\n",
    "    The logistic regression is fully described by a weight matrix :math:`W`\n",
    "    and bias vector :math:`b`. Classification is done by projecting data\n",
    "    points onto a set of hyperplanes, the distance to which is used to\n",
    "    determine a class membership probability.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        \"\"\" Initialize the parameters of the logistic regression\n",
    "\n",
    "        :type input: theano.tensor.TensorType\n",
    "        :param input: symbolic variable that describes the input of the\n",
    "                      architecture (one minibatch)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: number of input units, the dimension of the space in\n",
    "                     which the datapoints lie\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of output units, the dimension of the space in\n",
    "                      which the labels lie\n",
    "\n",
    "        \"\"\"\n",
    "        rng = numpy.random.RandomState(1234)\n",
    "\n",
    "        W_values = numpy.asarray(\n",
    "                rng.uniform(\n",
    "                    low=-4 * numpy.sqrt(6. / (n_in + n_out)),\n",
    "                    high=4 * numpy.sqrt(6. / (n_in + n_out)),\n",
    "                    size=(n_in, n_out)\n",
    "                ),\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "        \n",
    "        W = theano.shared(value=W_values, name='W', borrow=True)\n",
    "            \n",
    "        b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n",
    "        b = theano.shared(value=b_values, name='b', borrow=True)\n",
    "            \n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        # symbolic expression for computing the matrix of class-membership\n",
    "        # probabilities\n",
    "        # Where:\n",
    "        # W is a matrix where column-k represent the separation hyperplane for\n",
    "        # class-k\n",
    "        # x is a matrix where row-j  represents input training sample-j\n",
    "        # b is a vector where element-k represent the free parameter of\n",
    "        # hyperplane-k\n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
    "\n",
    "        # symbolic description of how to compute prediction as class whose121\n",
    "        # probability is maximal\n",
    "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
    "        # end-snippet-1\n",
    "\n",
    "        # parameters of the model\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "        # keep track of model input\n",
    "        self.input = input\n",
    "\n",
    "    def negative_log_likelihood(self, y):\n",
    "        \"\"\"Return the mean of the negative log-likelihood of the prediction\n",
    "        of this model under a given target distribution.\n",
    "\n",
    "        .. math::\n",
    "\n",
    "            \\frac{1}{|\\mathcal{D}|} \\mathcal{L} (\\theta=\\{W,b\\}, \\mathcal{D}) =\n",
    "            \\frac{1}{|\\mathcal{D}|} \\sum_{i=0}^{|\\mathcal{D}|}\n",
    "                \\log(P(Y=y^{(i)}|x^{(i)}, W,b)) \\\\\n",
    "            \\ell (\\theta=\\{W,b\\}, \\mathcal{D})\n",
    "\n",
    "        :type y: theano.tensor.TensorType\n",
    "        :param y: corresponds to a vector that gives for each example the\n",
    "                  correct label\n",
    "\n",
    "        Note: we use the mean instead of the sum so that\n",
    "              the learning rate is less dependent on the batch size\n",
    "        \"\"\"\n",
    "        # y.shape[0] is (symbolically) the number of rows in y, i.e.,\n",
    "        # number of examples (call it n) in the minibatch\n",
    "        # T.arange(y.shape[0]) is a symbolic vector which will contain\n",
    "        # [0,1,2,... n-1] T.log(self.p_y_given_x) is a matrix of\n",
    "        # Log-Probabilities (call it LP) with one row per example and\n",
    "        # one column per class LP[T.arange(y.shape[0]),y] is a vector\n",
    "        # v containing [LP[0,y[0]], LP[1,y[1]], LP[2,y[2]], ...,\n",
    "        # LP[n-1,y[n-1]]] and T.mean(LP[T.arange(y.shape[0]),y]) is\n",
    "        # the mean (across minibatch examples) of the elements in v,\n",
    "        # i.e., the mean log-likelihood across the minibatch.\n",
    "        \n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "#         return - T.sum(y * T.log(self.p_y_given_x) + (1 - y) * T.log(1 - self.p_y_given_x), axis=1) \n",
    "\n",
    "    def errors(self, y):\n",
    "        \"\"\"Return a float representing the number of errors in the minibatch\n",
    "        over the total number of examples of the minibatch ; zero one\n",
    "        loss over the size of the minibatch\n",
    "\n",
    "        :type y: theano.tensor.TensorType\n",
    "        :param y: corresponds to a vector that gives for each example the\n",
    "                  correct label\n",
    "        \"\"\"\n",
    "\n",
    "        # check if y has same dimension of y_pred\n",
    "        if y.ndim != self.y_pred.ndim:\n",
    "            raise TypeError(\n",
    "                'y should have the same shape as self.y_pred',\n",
    "                ('y', y.type, 'y_pred', self.y_pred.type)\n",
    "            )\n",
    "        # check if y is of the correct datatype\n",
    "        if y.dtype.startswith('int'):\n",
    "            # the T.neq operator returns a vector of 0s and 1s, where 1\n",
    "            # represents a mistake in prediction\n",
    "            return T.mean(T.neq(self.y_pred, y))\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def mse(self, batch_size, output_size, y):\n",
    "        \n",
    "        y2 = T.zeros([batch_size,output_size])\n",
    "        y2 = T.set_subtensor(y2[range(0,batch_size),y],1.0)\n",
    "\n",
    "        return T.sum(T.pow(self.p_y_given_x-y2, 2)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09. Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    test_mlp()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
